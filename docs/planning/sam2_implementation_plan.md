# SAM2ベースLive2D素材分割システム実装計画

## 📅 基本情報

- **日付**: 2025-06-24
- **移行理由**: YOLOの汎用物体検出からSAM2の高精度セグメンテーションへ転換
- **目標**: Live2D特化の細部分割システム構築

## 🎯 SAM2の優位性

### ✅ Live2D用途での利点

1. **プロンプト駆動**: 点・ボックス・テキストでの指定セグメンテーション
2. **高精度境界**: ピクセル単位での精密な境界検出
3. **柔軟性**: 事前学習なしで任意の領域を分割可能
4. **インタラクティブ**: ユーザー指定による部位別分割
5. **ファインチューニング不要**: そのまま実用可能

### ⚠️ YOLOとの比較

| 項目 | YOLO | SAM2 |
|------|------|------|
| 検出対象 | 事前定義クラス | 任意の領域 |
| Live2D適性 | 低（umbrella, clock等誤検出） | 高（正確な部位分割） |
| ユーザー制御 | なし | 点・ボックス指定可能 |
| 境界精度 | 中程度 | 非常に高精度 |
| 学習要否 | ファインチューニング必要 | 不要 |

## 🏗️ システムアーキテクチャ

### 1. コアモジュール設計

```
src/core/sam2/
├── sam2_model.py          # SAM2モデル管理
├── predictor.py           # セグメンテーション実行
├── prompt_handler.py      # プロンプト処理（点・ボックス・テキスト）
├── mask_processor.py      # マスク後処理・最適化
└── live2d_adapter.py      # Live2D特化処理
```

### 2. インタラクティブUI

```
src/ui/sam2_interface/
├── streamlit_app.py       # メインUI
├── canvas_widget.py       # 画像上での点・ボックス指定
├── prompt_manager.py      # プロンプト管理
└── result_viewer.py       # 結果表示・編集
```

### 3. プロンプト戦略

#### Live2D部位別プロンプト
- **髪**: 髪の中心点 + "hair" テキスト
- **顔**: 顔の境界ボックス + "face" テキスト  
- **目**: 各目の中心点 + "eye" テキスト
- **角・アクセサリー**: 手動点指定
- **体・服**: 領域ボックス指定

## 📋 実装フェーズ

### Phase A: SAM2基盤構築（1-2日）

**目標**: SAM2の基本動作確認

1. **環境構築**
   - SAM2ライブラリインストール
   - モデルダウンロード（sam2_hiera_large.pt推奨）
   - GPU対応確認

2. **基本クラス実装**
   - SAM2ModelManager: モデル読み込み・管理
   - SAM2Predictor: 基本セグメンテーション
   - SimplePromptHandler: 点・ボックスプロンプト

3. **テスト**
   - サンプル画像での点指定セグメンテーション
   - ボックス指定セグメンテーション
   - 結果精度確認

### Phase B: Live2D特化機能（2-3日）

**目標**: Live2D用途での実用性確保

1. **Live2D Adapter実装**
   - 部位別プロンプトテンプレート
   - キャラクター特化の後処理
   - レイヤー分離ロジック

2. **マスク最適化**
   - 境界スムージング
   - 小領域ノイズ除去
   - 透明度処理

3. **バッチ処理**
   - 複数部位の自動分割
   - プロンプトセット保存・読み込み

### Phase C: インタラクティブUI（2-3日）

**目標**: 使いやすいユーザーインターフェース

1. **Canvas Widget**
   - 画像上での点・ボックス指定
   - リアルタイムプレビュー
   - プロンプト編集・削除

2. **Streamlit統合**
   - ファイルアップロード
   - プロンプト管理UI
   - 結果ダウンロード

3. **結果編集機能**
   - マスク手動調整
   - レイヤー名変更
   - 出力形式選択

## 🔧 技術要件

### 必要ライブラリ

```python
# SAM2関連
segment-anything-2>=1.0.0
torch>=2.0.0
torchvision>=0.15.0
pillow>=10.0.0

# UI関連  
streamlit>=1.28.0
streamlit-drawable-canvas>=0.9.0

# 画像処理
opencv-python>=4.8.0
numpy>=1.24.0
scipy>=1.11.0

# その他
pyyaml>=6.0.0
tqdm>=4.66.0
```

### 推奨ハードウェア
- **GPU**: RTX 2060以上（6GB VRAM）
- **RAM**: 16GB以上
- **ストレージ**: モデルファイル用に5GB

## 🎮 ユーザーワークフロー

### 1. 基本ワークフロー

```
1. 画像アップロード
   ↓
2. キャラクター全体を大まかに指定（ボックス）
   ↓  
3. 部位別詳細指定
   - 髪: 点クリック
   - 顔: 境界ボックス
   - 目: 各目に点
   - アクセサリー: 手動指定
   ↓
4. SAM2実行・結果確認
   ↓
5. 手動調整（必要に応じて）
   ↓
6. Live2Dレイヤーとして出力
```

### 2. プロンプト戦略例

**ドット絵女の子（角・鎖付き）の場合**:
- ボックス1: キャラクター全体
- 点1: 髪の中心
- 点2: 左角
- 点3: 右角  
- 点4: 鎖の中心
- ボックス2: 顔領域
- 点5: 左目
- 点6: 右目

## 📊 期待される改善点

### YOLOからの改善

1. **検出精度**: umbrella/clock誤検出 → 正確な部位分割
2. **ユーザー制御**: 自動のみ → インタラクティブ指定
3. **境界品質**: 粗い境界 → ピクセル精度
4. **適用範囲**: COCO限定 → 任意の画像・部位
5. **カスタマイズ**: 学習必要 → 即座に適用可能

### Live2D適性向上

- **髪**: 個別の房・レイヤー分割
- **アクセサリー**: 角・鎖・装飾品の精密分割
- **表情**: 目・口・眉の細部制御
- **服装**: 重なり・透明度考慮

## 🚨 リスク・課題

### 技術的課題

1. **処理速度**: SAM2はYOLOより重い（対策: モデル最適化）
2. **メモリ使用量**: 大きな画像での制限（対策: タイル処理）
3. **プロンプト設計**: 最適なプロンプト戦略要検討

### UX課題

1. **学習コスト**: ユーザーの点・ボックス指定スキル
2. **反復作業**: 複数部位の手動指定
3. **プレビュー速度**: リアルタイム確認の実装

## 🎯 成功指標

### Phase A成功基準
- [x] SAM2環境構築完了
- [ ] 基本セグメンテーション動作
- [ ] GPU対応確認

### Phase B成功基準  
- [ ] Live2D部位の正確な分割
- [ ] 境界品質がYOLOを上回る
- [ ] バッチ処理による効率化

### Phase C成功基準
- [ ] 直感的なUI操作
- [ ] 5分以内での基本分割完了
- [ ] Live2D用ファイル出力

## 📈 次期展開

### 将来機能
1. **AIアシスト**: よく使用されるプロンプトパターンの学習
2. **テンプレート**: キャラクタータイプ別プロンプトセット
3. **自動化**: 顔検出→自動プロンプト生成
4. **品質向上**: エッジ検出・スムージング追加
5. **コラボ機能**: 複数ユーザーでのプロンプト共有

この計画に基づいて、まずPhase Aから開始することを推奨します。