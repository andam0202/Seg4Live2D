#!/usr/bin/env python3
"""
SAM2æœ€é©ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ

æœ€è‰¯ã®æˆ¦ç•¥ï¼ˆAdaptive Sparse + ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ï¼‰ã§å„ãƒ‘ãƒ¼ãƒ„ã‚’åˆ†å‰²ã—ã€
çµæœã‚’ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒã¨å…±ã«ä¿å­˜ã™ã‚‹
"""

import sys
import cv2
import numpy as np
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
import json
import shutil

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.utils import setup_logging, get_logger
from src.core.sam2.sam2_model import SAM2ModelManager
from src.core.sam2.prompt_handler import SAM2PromptHandler, PointPrompt

# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from scripts.test_sam2_hybrid_enhancement import SAM2HybridEnhancer

@dataclass
class SegmentationResult:
    """ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœ"""
    part_name: str
    mask: np.ndarray
    score: float
    processing_time: float
    prompts_used: int
    strategy: str
    enhancement_log: Dict[str, Any]

class OptimalSegmenter:
    """æœ€é©ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self, sam2_manager: SAM2ModelManager):
        self.sam2_manager = sam2_manager
        self.hybrid_enhancer = SAM2HybridEnhancer(sam2_manager)
        self.logger = get_logger(__name__)
        
    def generate_adaptive_sparse_prompts(self, image_shape: Tuple[int, int], part_name: str) -> List[PointPrompt]:
        """é©å¿œçš„ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆæœ€å„ªç§€æˆ¦ç•¥ï¼‰"""
        height, width = image_shape[:2]
        
        sparse_prompts = {
            "hair": [
                # æˆ¦ç•¥çš„æœ€å°ç‚¹é…ç½®
                PointPrompt(width//2, height//6, 1, "é ­é ‚éƒ¨_é«ª"),
                PointPrompt(width//4, height//2, 1, "å·¦ã‚µã‚¤ãƒ‰_é«ª"),
                PointPrompt(3*width//4, height//2, 1, "å³ã‚µã‚¤ãƒ‰_é«ª"),
                PointPrompt(width//2, 2*height//3, 1, "å¾Œé«ª_æµã‚Œ"),
                # é‡è¦ãªé™¤å¤–ç‚¹
                PointPrompt(width//2, 2*height//5, 0, "é¡”_é™¤å¤–"),
                PointPrompt(width//2, 4*height//5, 0, "ä½“_é™¤å¤–"),
            ],
            "face": [
                PointPrompt(width//2, 2*height//5, 1, "é¡”_ä¸­å¿ƒ"),
                PointPrompt(width//3, height//2, 1, "å·¦é ¬_è‚Œ"),
                PointPrompt(2*width//3, height//2, 1, "å³é ¬_è‚Œ"),
                PointPrompt(width//2, height//6, 0, "é«ª_é™¤å¤–"),
                PointPrompt(width//6, height//2, 0, "èƒŒæ™¯_é™¤å¤–"),
            ],
            "body": [
                PointPrompt(width//2, 3*height//5, 1, "èƒ´ä½“_ä¸­å¿ƒ"),
                PointPrompt(width//3, 2*height//3, 1, "å·¦è‚©_ä½“"),
                PointPrompt(2*width//3, 2*height//3, 1, "å³è‚©_ä½“"),
                PointPrompt(width//2, height//3, 0, "é¦–ä¸Š_é™¤å¤–"),
            ],
            "eyes": [
                PointPrompt(width//3, 2*height//5, 1, "å·¦çœ¼_ç³"),
                PointPrompt(2*width//3, 2*height//5, 1, "å³çœ¼_ç³"),
                PointPrompt(width//2, height//3, 0, "é¡_é™¤å¤–"),
            ]
        }
        
        prompts = sparse_prompts.get(part_name, sparse_prompts["hair"])
        self.logger.info(f"  âš¡ é©å¿œçš„ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ: {len(prompts)}ç‚¹")
        return prompts
    
    def segment_part(self, image_rgb: np.ndarray, part_name: str) -> SegmentationResult:
        """ãƒ‘ãƒ¼ãƒ„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        self.logger.info(f"ğŸ¯ {part_name.upper()}ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")
        
        start_time = time.time()
        height, width = image_rgb.shape[:2]
        
        # é©å¿œçš„ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompts = self.generate_adaptive_sparse_prompts((height, width), part_name)
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã§å®Ÿè¡Œ
        best_mask, best_score, enhancement_log = self.hybrid_enhancer.hybrid_enhancement(
            image_rgb, prompts, part_name
        )
        
        processing_time = time.time() - start_time
        
        result = SegmentationResult(
            part_name=part_name,
            mask=best_mask,
            score=best_score,
            processing_time=processing_time,
            prompts_used=len(prompts),
            strategy="adaptive_sparse_hybrid",
            enhancement_log=enhancement_log
        )
        
        self.logger.info(f"  âœ… å®Œäº†: ã‚¹ã‚³ã‚¢ {best_score:.3f}, æ™‚é–“ {processing_time:.2f}s")
        return result
    
    def create_visualization(self, image_rgb: np.ndarray, result: SegmentationResult) -> np.ndarray:
        """çµæœå¯è¦–åŒ–"""
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ä½œæˆ
        overlay = image_rgb.copy().astype(np.float32)
        
        # ãƒ‘ãƒ¼ãƒ„åˆ¥è‰²åˆ†ã‘
        colors = {
            "hair": [255, 100, 150],    # ãƒ”ãƒ³ã‚¯
            "face": [255, 200, 100],    # ã‚ªãƒ¬ãƒ³ã‚¸
            "body": [100, 200, 255],    # é’
            "eyes": [150, 255, 100],    # ç·‘
        }
        color = np.array(colors.get(result.part_name, [255, 255, 255]))
        
        # ãƒã‚¹ã‚¯é©ç”¨
        mask_bool = result.mask.astype(bool)
        overlay[mask_bool] = overlay[mask_bool] * 0.4 + color * 0.6
        
        # æƒ…å ±è¡¨ç¤º
        info_lines = [
            f"Part: {result.part_name.upper()}",
            f"Score: {result.score:.3f}",
            f"Strategy: {result.strategy}",
            f"Time: {result.processing_time:.2f}s",
            f"Prompts: {result.prompts_used}",
        ]
        
        for i, line in enumerate(info_lines):
            y_pos = 30 + i * 25
            # é»’ã„èƒŒæ™¯
            text_size = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(overlay.astype(np.uint8), (10, y_pos - 20), 
                         (text_size[0] + 20, y_pos + 5), (0, 0, 0), -1)
            # ç™½ã„æ–‡å­—
            cv2.putText(overlay.astype(np.uint8), line, (15, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return overlay.astype(np.uint8)
    
    def save_results(self, image_rgb: np.ndarray, results: List[SegmentationResult], 
                    output_dir: Path, image_name: str) -> Dict[str, Any]:
        """çµæœä¿å­˜"""
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒä¿å­˜
        original_path = output_dir / f"original_{image_name}.png"
        original_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
        cv2.imwrite(str(original_path), original_bgr)
        self.logger.info(f"  ğŸ“¸ ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒä¿å­˜: {original_path.name}")
        
        saved_files = {"original": str(original_path)}
        
        # ãƒ‘ãƒ¼ãƒ„åˆ¥çµæœä¿å­˜
        for result in results:
            part_name = result.part_name
            
            # ãƒã‚¹ã‚¯ç”»åƒä¿å­˜ï¼ˆãƒã‚¤ãƒŠãƒªï¼‰
            mask_path = output_dir / f"mask_{part_name}_{image_name}.png"
            cv2.imwrite(str(mask_path), (result.mask * 255).astype(np.uint8))
            saved_files[f"mask_{part_name}"] = str(mask_path)
            
            # é€æ˜PNGä¿å­˜ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒãƒ£ãƒ³ãƒãƒ«ä»˜ãï¼‰
            transparent_path = output_dir / f"transparent_{part_name}_{image_name}.png"
            transparent_image = image_rgb.copy()
            alpha_channel = result.mask.astype(np.uint8) * 255
            transparent_rgba = np.dstack([transparent_image, alpha_channel])
            transparent_bgra = cv2.cvtColor(transparent_rgba, cv2.COLOR_RGBA2BGRA)
            cv2.imwrite(str(transparent_path), transparent_bgra)
            saved_files[f"transparent_{part_name}"] = str(transparent_path)
            
            # å¯è¦–åŒ–ç”»åƒä¿å­˜
            vis_image = self.create_visualization(image_rgb, result)
            vis_path = output_dir / f"visualization_{part_name}_{image_name}.png"
            vis_bgr = cv2.cvtColor(vis_image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(str(vis_path), vis_bgr)
            saved_files[f"visualization_{part_name}"] = str(vis_path)
        
        # å…¨ãƒ‘ãƒ¼ãƒ„çµ±åˆç”»åƒä½œæˆ
        combined_image = self.create_combined_visualization(image_rgb, results)
        combined_path = output_dir / f"combined_all_parts_{image_name}.png"
        combined_bgr = cv2.cvtColor(combined_image, cv2.COLOR_RGB2BGR)
        cv2.imwrite(str(combined_path), combined_bgr)
        saved_files["combined"] = str(combined_path)
        
        self.logger.info(f"  ğŸ¨ çµ±åˆç”»åƒä¿å­˜: {combined_path.name}")
        
        return saved_files
    
    def create_combined_visualization(self, image_rgb: np.ndarray, 
                                    results: List[SegmentationResult]) -> np.ndarray:
        """å…¨ãƒ‘ãƒ¼ãƒ„çµ±åˆå¯è¦–åŒ–"""
        overlay = image_rgb.copy().astype(np.float32)
        
        # ãƒ‘ãƒ¼ãƒ„åˆ¥è‰²åˆ†ã‘
        colors = {
            "hair": [255, 100, 150],    # ãƒ”ãƒ³ã‚¯
            "face": [255, 200, 100],    # ã‚ªãƒ¬ãƒ³ã‚¸  
            "body": [100, 200, 255],    # é’
            "eyes": [150, 255, 100],    # ç·‘
        }
        
        # å„ãƒ‘ãƒ¼ãƒ„ã®ãƒã‚¹ã‚¯ã‚’é‡ã­åˆã‚ã›
        for result in results:
            color = np.array(colors.get(result.part_name, [255, 255, 255]))
            mask_bool = result.mask.astype(bool)
            overlay[mask_bool] = overlay[mask_bool] * 0.7 + color * 0.3
        
        # çµ±åˆæƒ…å ±è¡¨ç¤º
        total_score = np.mean([r.score for r in results])
        total_time = sum([r.processing_time for r in results])
        
        info_lines = [
            "All Parts Segmentation",
            f"Parts: {len(results)}",
            f"Avg Score: {total_score:.3f}",
            f"Total Time: {total_time:.2f}s",
            f"Strategy: adaptive_sparse_hybrid",
        ]
        
        for i, line in enumerate(info_lines):
            y_pos = 30 + i * 25
            text_size = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
            cv2.rectangle(overlay.astype(np.uint8), (10, y_pos - 20), 
                         (text_size[0] + 20, y_pos + 5), (0, 0, 0), -1)
            cv2.putText(overlay.astype(np.uint8), line, (15, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        return overlay.astype(np.uint8)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # ãƒ­ã‚°è¨­å®š
    setup_logging(level="INFO", console_output=True, structured=False)
    logger = get_logger(__name__)
    
    logger.info("=== SAM2æœ€é©ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ ===")
    
    try:
        # æœ¬ç•ªç”»åƒå–å¾—
        sample_images_path = project_root / "data" / "samples" / "demo_images2"
        image_files = list(sample_images_path.glob("*.png"))
        
        if not image_files:
            logger.error("âŒ æœ¬ç•ªç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (data/samples/demo_images2/)")
            return
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
        output_dir = project_root / "data" / "output" / "sam2_optimal_segmentation"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # SAM2ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        logger.info("ğŸ¤– SAM2ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–")
        sam2_manager = SAM2ModelManager(model_name="sam2_hiera_large.pt")
        if not sam2_manager.load_model():
            logger.error("âŒ SAM2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—")
            return
        
        segmenter = OptimalSegmenter(sam2_manager)
        
        # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ‘ãƒ¼ãƒ„ï¼ˆæœ€é©é †åºï¼‰
        target_parts = ["face", "body", "hair", "eyes"]  # Adaptive SparseãŒæœ€ã‚‚å¾—æ„ãªé †åº
        
        # å„ç”»åƒã§å®Ÿè¡Œ
        for image_path in image_files:
            logger.info(f"\nğŸ“¸ ç”»åƒå‡¦ç†é–‹å§‹: {image_path.name}")
            
            # ç”»åƒèª­ã¿è¾¼ã¿
            image = cv2.imread(str(image_path))
            if image is None:
                logger.warning(f"  âš ï¸ ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {image_path}")
                continue
                
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            logger.info(f"  ğŸ“ ç”»åƒã‚µã‚¤ã‚º: {image_rgb.shape[1]}Ã—{image_rgb.shape[0]}")
            
            # å„ãƒ‘ãƒ¼ãƒ„ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            results = []
            total_start_time = time.time()
            
            for part_name in target_parts:
                logger.info(f"\n  ğŸ¯ {part_name.upper()}ãƒ‘ãƒ¼ãƒ„å‡¦ç†")
                
                try:
                    result = segmenter.segment_part(image_rgb, part_name)
                    results.append(result)
                    
                except Exception as e:
                    logger.error(f"    âŒ {part_name}ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—: {e}")
                    continue
            
            total_processing_time = time.time() - total_start_time
            
            if not results:
                logger.warning(f"  âš ï¸ {image_path.name}ã®å‡¦ç†çµæœãŒã‚ã‚Šã¾ã›ã‚“")
                continue
            
            # çµæœä¿å­˜
            logger.info(f"\n  ğŸ’¾ çµæœä¿å­˜ä¸­...")
            saved_files = segmenter.save_results(image_rgb, results, output_dir, image_path.stem)
            
            # è©³ç´°æƒ…å ±JSONä¿å­˜
            results_json = {
                "image_name": image_path.name,
                "image_size": {"width": image_rgb.shape[1], "height": image_rgb.shape[0]},
                "total_processing_time": total_processing_time,
                "parts_processed": len(results),
                "strategy": "adaptive_sparse_hybrid",
                "results": {
                    result.part_name: {
                        "score": result.score,
                        "processing_time": result.processing_time,
                        "prompts_used": result.prompts_used,
                        "strategy": result.strategy
                    } for result in results
                },
                "saved_files": saved_files,
                "summary": {
                    "average_score": np.mean([r.score for r in results]),
                    "total_time": total_processing_time,
                    "best_part": max(results, key=lambda r: r.score).part_name,
                    "best_score": max(results, key=lambda r: r.score).score
                }
            }
            
            json_path = output_dir / f"results_{image_path.stem}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(results_json, f, indent=2, ensure_ascii=False)
            
            # çµæœã‚µãƒãƒªãƒ¼
            logger.info(f"\n  ğŸ“Š {image_path.name} å‡¦ç†å®Œäº†:")
            logger.info(f"    ğŸ¯ å‡¦ç†ãƒ‘ãƒ¼ãƒ„: {len(results)}")
            logger.info(f"    ğŸ“ˆ å¹³å‡ã‚¹ã‚³ã‚¢: {np.mean([r.score for r in results]):.3f}")
            logger.info(f"    â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_processing_time:.2f}ç§’")
            logger.info(f"    ğŸ† æœ€é«˜ã‚¹ã‚³ã‚¢: {max(results, key=lambda r: r.score).score:.3f} ({max(results, key=lambda r: r.score).part_name})")
            logger.info(f"    ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(saved_files)}")
        
        # å…¨ä½“ã‚µãƒãƒªãƒ¼
        logger.info(f"\nğŸ‰ å…¨ç”»åƒå‡¦ç†å®Œäº†!")
        logger.info(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}")
        logger.info(f"ğŸ–¼ï¸ å‡¦ç†ç”»åƒæ•°: {len(list(output_dir.glob('original_*.png')))}")
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª¬æ˜
        logger.info(f"\nğŸ“‹ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª¬æ˜:")
        logger.info(f"  ğŸ“¸ original_*.png: ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒ")
        logger.info(f"  ğŸ­ mask_*.png: ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ï¼ˆç™½é»’ï¼‰")
        logger.info(f"  ğŸŒŸ transparent_*.png: é€æ˜PNGï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒãƒ£ãƒ³ãƒãƒ«ä»˜ãï¼‰")
        logger.info(f"  ğŸ¨ visualization_*.png: å¯è¦–åŒ–ç”»åƒï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ï¼‰")
        logger.info(f"  ğŸ¯ combined_all_parts_*.png: å…¨ãƒ‘ãƒ¼ãƒ„çµ±åˆç”»åƒ")
        logger.info(f"  ğŸ“‹ results_*.json: è©³ç´°çµæœãƒ‡ãƒ¼ã‚¿")
        
    except Exception as e:
        logger.error(f"âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if 'sam2_manager' in locals():
            sam2_manager.unload_model()

if __name__ == "__main__":
    main()