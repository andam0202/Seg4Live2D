#!/usr/bin/env python3
"""
SAM2 Live2Dãƒ‘ãƒ¼ãƒ„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

Live2Dåˆ¶ä½œç”¨ã®é«˜ç²¾åº¦ãƒ‘ãƒ¼ãƒ„åˆ†å‰²ã‚·ã‚¹ãƒ†ãƒ 
- é«ªã€é¡”ã€ä½“ã€ç›®ã®è‡ªå‹•åˆ†å‰²
- é€æ˜PNGå‡ºåŠ›ã§Live2Då³åˆ©ç”¨å¯èƒ½
- argparseå¯¾å¿œã§æŸ”è»Ÿãªå®Ÿè¡Œã‚ªãƒ—ã‚·ãƒ§ãƒ³
"""

import sys
import cv2
import numpy as np
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.utils import setup_logging, get_logger
from src.core.sam2.sam2_model import SAM2ModelManager
from src.core.sam2.prompt_handler import PointPrompt

@dataclass
class SegmentationResult:
    """ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœ"""
    part_name: str
    mask: np.ndarray
    score: float
    processing_time: float

class Live2DSegmenter:
    """Live2Dç”¨ãƒ‘ãƒ¼ãƒ„ã‚»ã‚°ãƒ¡ãƒ³ã‚¿ãƒ¼"""
    
    def __init__(self, sam2_manager: SAM2ModelManager):
        self.sam2_manager = sam2_manager
        self.logger = get_logger(__name__)
        
    def get_optimized_prompts(self, image_shape: Tuple[int, int], part_name: str) -> List[PointPrompt]:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        height, width = image_shape[:2]
        
        # å®Ÿé¨“ã§æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé…ç½®
        prompts = {
            "hair": [
                # é«ªã®ä¸­æ ¸éƒ¨åˆ†ï¼ˆè§’ã‚’é™¤å¤–ï¼‰
                PointPrompt(width//4, height//6, 1, "å·¦é«ª_å†…å´"),
                PointPrompt(3*width//4, height//6, 1, "å³é«ª_å†…å´"),
                PointPrompt(width//2, height//10, 1, "å¾Œé«ª_ä¸­å¤®"),
                # å¼·åŠ›ãªé™¤å¤–
                PointPrompt(width//2, height//20, 0, "è§’_é™¤å¤–"),
                PointPrompt(width//2, height//3, 0, "é¡”_é™¤å¤–"),
                PointPrompt(width//2, 3*height//5, 0, "ä½“_é™¤å¤–"),
            ],
            "face": [
                # é¡”ã®è‚Œéƒ¨åˆ†ã®ã¿
                PointPrompt(width//2, height//2, 1, "é¡”_ä¸­å¤®"),
                PointPrompt(2*width//5, height//2, 1, "å·¦é ¬"),
                PointPrompt(3*width//5, height//2, 1, "å³é ¬"),
                PointPrompt(width//2, 3*height//7, 1, "é¡_ä¸‹éƒ¨"),
                # é«ªã¨ä½“ã‚’é™¤å¤–
                PointPrompt(width//2, height//4, 0, "å‰é«ª_é™¤å¤–"),
                PointPrompt(width//2, 3*height//5, 0, "é¦–_é™¤å¤–"),
                PointPrompt(width//6, height//2, 0, "èƒŒæ™¯_é™¤å¤–"),
            ],
            "body": [
                # é¦–ã‹ã‚‰ä¸‹ã®ä½“
                PointPrompt(width//2, 2*height//3, 1, "èƒ´ä½“_ä¸­å¤®"),
                PointPrompt(2*width//5, 3*height//4, 1, "å·¦è‚©"),
                PointPrompt(3*width//5, 3*height//4, 1, "å³è‚©"),
                # é¡”ã‚’é™¤å¤–
                PointPrompt(width//2, height//2, 0, "é¡”_é™¤å¤–"),
                PointPrompt(width//2, height//3, 0, "é¦–ä¸Š_é™¤å¤–"),
                PointPrompt(width//6, 2*height//3, 0, "èƒŒæ™¯_é™¤å¤–"),
            ],
            "eyes": [
                # ç›®ã®éƒ¨åˆ†ã®ã¿ï¼ˆæœ€é«˜åŠ¹ç‡æ€§é”æˆæ¸ˆã¿ï¼‰
                PointPrompt(2*width//5, 2*height//5, 1, "å·¦ç›®"),
                PointPrompt(3*width//5, 2*height//5, 1, "å³ç›®"),
                # å³æ ¼ãªé™¤å¤–
                PointPrompt(width//2, height//3, 0, "é¡_é™¤å¤–"),
                PointPrompt(width//2, height//4, 0, "å‰é«ª_é™¤å¤–"),
                PointPrompt(width//2, height//2, 0, "é¼»_é™¤å¤–"),
                PointPrompt(width//4, 2*height//5, 0, "å·¦é ¬_é™¤å¤–"),
                PointPrompt(3*width//4, 2*height//5, 0, "å³é ¬_é™¤å¤–"),
            ]
        }
        
        return prompts.get(part_name, prompts["hair"])
    
    def segment_part(self, image_rgb: np.ndarray, part_name: str) -> SegmentationResult:
        """ãƒ‘ãƒ¼ãƒ„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        self.logger.info(f"ğŸ¯ {part_name.upper()}ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")
        
        start_time = time.time()
        prompts = self.get_optimized_prompts(image_rgb.shape, part_name)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™
        point_coords = [[p.x, p.y] for p in prompts]
        point_labels = [p.label for p in prompts]
        
        # SAM2å®Ÿè¡Œ
        masks, scores, _ = self.sam2_manager.predict(
            image=image_rgb,
            point_coords=np.array(point_coords),
            point_labels=np.array(point_labels),
            multimask_output=True
        )
        
        # æœ€è‰¯çµæœé¸æŠ
        best_idx = np.argmax(scores)
        processing_time = time.time() - start_time
        
        result = SegmentationResult(
            part_name=part_name,
            mask=masks[best_idx],
            score=scores[best_idx],
            processing_time=processing_time
        )
        
        coverage = np.sum(result.mask) / result.mask.size * 100
        self.logger.info(f"  âœ… å®Œäº†: ã‚¹ã‚³ã‚¢ {result.score:.3f}, ã‚«ãƒãƒ¼ç‡ {coverage:.1f}%")
        
        return result
    
    def create_visualization(self, image_rgb: np.ndarray, result: SegmentationResult) -> np.ndarray:
        """å¯è¦–åŒ–ç”»åƒä½œæˆ"""
        vis_image = image_rgb.copy()
        
        # ãƒ‘ãƒ¼ãƒ„åˆ¥è‰²åˆ†ã‘
        colors = {
            "hair": [255, 100, 150],    # ãƒ”ãƒ³ã‚¯
            "face": [255, 200, 100],    # ã‚ªãƒ¬ãƒ³ã‚¸
            "body": [100, 200, 255],    # é’
            "eyes": [150, 255, 100],    # ç·‘
        }
        color = np.array(colors.get(result.part_name, [255, 255, 255]))
        
        # ãƒã‚¹ã‚¯é©ç”¨ã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤
        mask_bool = result.mask.astype(bool)
        vis_image[mask_bool] = vis_image[mask_bool] * 0.6 + color * 0.4
        
        # å¢ƒç•Œç·šæç”»
        mask_uint8 = (result.mask * 255).astype(np.uint8)
        contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(vis_image, contours, -1, color.tolist(), 2)
        
        # æƒ…å ±è¡¨ç¤º
        coverage = np.sum(result.mask) / result.mask.size * 100
        info_lines = [
            f"Part: {result.part_name.upper()}",
            f"Score: {result.score:.3f}",
            f"Time: {result.processing_time:.2f}s",
            f"Coverage: {coverage:.1f}%",
        ]
        
        for i, line in enumerate(info_lines):
            y = 30 + i * 30
            cv2.putText(vis_image, line, (15, y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        return vis_image
    
    def save_results(self, image_rgb: np.ndarray, results: List[SegmentationResult], 
                    output_dir: Path, image_name: str) -> Dict[str, str]:
        """çµæœä¿å­˜ï¼ˆLive2Dç”¨ï¼‰"""
        saved_files = {}
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒ
        original_path = output_dir / f"original_{image_name}.png"
        cv2.imwrite(str(original_path), cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR))
        saved_files["original"] = str(original_path)
        
        for result in results:
            part = result.part_name
            
            # ãƒã‚¹ã‚¯ï¼ˆç™½é»’ï¼‰
            mask_path = output_dir / f"mask_{part}_{image_name}.png"
            cv2.imwrite(str(mask_path), (result.mask * 255).astype(np.uint8))
            saved_files[f"mask_{part}"] = str(mask_path)
            
            # é€æ˜PNGï¼ˆLive2Dç”¨ï¼‰
            transparent_path = output_dir / f"live2d_{part}_{image_name}.png"
            alpha = result.mask.astype(np.uint8) * 255
            rgba = np.dstack([image_rgb, alpha])
            bgra = cv2.cvtColor(rgba, cv2.COLOR_RGBA2BGRA)
            cv2.imwrite(str(transparent_path), bgra)
            saved_files[f"live2d_{part}"] = str(transparent_path)
            
            # å¯è¦–åŒ–
            viz_path = output_dir / f"viz_{part}_{image_name}.png"
            viz_image = self.create_visualization(image_rgb, result)
            cv2.imwrite(str(viz_path), cv2.cvtColor(viz_image, cv2.COLOR_RGB2BGR))
            saved_files[f"viz_{part}"] = str(viz_path)
        
        return saved_files

def parse_args():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°è§£æ"""
    parser = argparse.ArgumentParser(
        description="SAM2 Live2Dãƒ‘ãƒ¼ãƒ„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # åŸºæœ¬å®Ÿè¡Œ
  python scripts/sam2_segmentation.py --input data/samples/anime_woman1
  
  # ç‰¹å®šãƒ‘ãƒ¼ãƒ„ã®ã¿
  python scripts/sam2_segmentation.py --input data/samples/anime_woman1 --parts face hair
  
  # å‡ºåŠ›å…ˆæŒ‡å®š
  python scripts/sam2_segmentation.py --input data/samples/anime_woman1 --output my_output
  
  # JPEGãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å‡¦ç†
  python scripts/sam2_segmentation.py --input data/samples/photos --pattern "*.jpg"
        """
    )
    
    parser.add_argument("--input", "-i", required=True, help="å…¥åŠ›ç”»åƒãƒ•ã‚©ãƒ«ãƒ€")
    parser.add_argument("--output", "-o", default="data/output/live2d_segmentation", help="å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€")
    parser.add_argument("--parts", "-p", nargs="+", choices=["face", "hair", "body", "eyes"], 
                       default=["face", "hair", "body", "eyes"], help="å¯¾è±¡ãƒ‘ãƒ¼ãƒ„")
    parser.add_argument("--pattern", default="*.png", help="ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³")
    parser.add_argument("--max-images", type=int, help="æœ€å¤§å‡¦ç†ç”»åƒæ•°")
    parser.add_argument("--verbose", "-v", action="store_true", help="è©³ç´°ãƒ­ã‚°")
    
    return parser.parse_args()

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    args = parse_args()
    
    # ãƒ­ã‚°è¨­å®š
    setup_logging(level="DEBUG" if args.verbose else "INFO", console_output=True, structured=False)
    logger = get_logger(__name__)
    
    logger.info("ğŸ¨ SAM2 Live2Dãƒ‘ãƒ¼ãƒ„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")
    logger.info(f"ğŸ“ å…¥åŠ›: {args.input}")
    logger.info(f"ğŸ“ å‡ºåŠ›: {args.output}")
    logger.info(f"ğŸ¯ ãƒ‘ãƒ¼ãƒ„: {', '.join(args.parts)}")
    
    try:
        # ãƒ‘ã‚¹è¨­å®š
        input_path = Path(args.input)
        if not input_path.is_absolute():
            input_path = project_root / input_path
            
        output_path = Path(args.output)
        if not output_path.is_absolute():
            output_path = project_root / output_path
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ç”»åƒå–å¾—
        image_files = list(input_path.glob(args.pattern))
        if args.max_images:
            image_files = image_files[:args.max_images]
            
        if not image_files:
            logger.error(f"âŒ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}/{args.pattern}")
            return
            
        logger.info(f"ğŸ“¸ å‡¦ç†ç”»åƒæ•°: {len(image_files)}")
        
        # SAM2åˆæœŸåŒ–
        logger.info("ğŸ¤– SAM2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        sam2_manager = SAM2ModelManager(model_name="sam2_hiera_large.pt")
        if not sam2_manager.load_model():
            logger.error("âŒ SAM2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—")
            return
            
        segmenter = Live2DSegmenter(sam2_manager)
        
        # çµ±è¨ˆ
        total_time = 0
        all_results = []
        processed_count = 0
        
        # ç”»åƒå‡¦ç†
        for i, image_path in enumerate(image_files):
            logger.info(f"\nğŸ“¸ å‡¦ç†ä¸­ ({i+1}/{len(image_files)}): {image_path.name}")
            
            try:
                # ç”»åƒèª­ã¿è¾¼ã¿
                image = cv2.imread(str(image_path))
                if image is None:
                    logger.warning(f"âš ï¸ èª­ã¿è¾¼ã¿å¤±æ•—: {image_path}")
                    continue
                    
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                
                # ãƒ‘ãƒ¼ãƒ„å‡¦ç†
                image_results = []
                image_start = time.time()
                
                for part in args.parts:
                    try:
                        result = segmenter.segment_part(image_rgb, part)
                        image_results.append(result)
                        all_results.append(result)
                    except Exception as e:
                        logger.error(f"âŒ {part}å‡¦ç†å¤±æ•—: {e}")
                
                if not image_results:
                    continue
                    
                # çµæœä¿å­˜
                saved_files = segmenter.save_results(image_rgb, image_results, output_path, image_path.stem)
                
                image_time = time.time() - image_start
                total_time += image_time
                processed_count += 1
                
                # ç”»åƒã‚µãƒãƒªãƒ¼
                avg_score = np.mean([r.score for r in image_results])
                logger.info(f"âœ… å®Œäº†: å¹³å‡ã‚¹ã‚³ã‚¢ {avg_score:.3f}, æ™‚é–“ {image_time:.1f}s")
                
            except Exception as e:
                logger.error(f"âŒ {image_path.name}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # å…¨ä½“ã‚µãƒãƒªãƒ¼
        if all_results:
            logger.info(f"\nğŸ‰ å‡¦ç†å®Œäº†!")
            logger.info(f"ğŸ“Š çµ±è¨ˆ:")
            logger.info(f"  å‡¦ç†ç”»åƒ: {processed_count}/{len(image_files)}")
            logger.info(f"  ç·ãƒ‘ãƒ¼ãƒ„: {len(all_results)}")
            logger.info(f"  å¹³å‡ã‚¹ã‚³ã‚¢: {np.mean([r.score for r in all_results]):.3f}")
            logger.info(f"  ç·æ™‚é–“: {total_time:.1f}ç§’")
            
            # ãƒ‘ãƒ¼ãƒ„åˆ¥çµ±è¨ˆ
            for part in args.parts:
                part_results = [r for r in all_results if r.part_name == part]
                if part_results:
                    avg_score = np.mean([r.score for r in part_results])
                    logger.info(f"  {part}: {len(part_results)}å€‹, å¹³å‡ã‚¹ã‚³ã‚¢ {avg_score:.3f}")
            
            logger.info(f"\nğŸ“ çµæœ: {output_path}")
            logger.info("ğŸ’¡ live2d_*.png ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Live2Dã§ä½¿ç”¨ã—ã¦ãã ã•ã„")
        else:
            logger.warning("âš ï¸ å‡¦ç†çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if 'sam2_manager' in locals():
            sam2_manager.unload_model()

if __name__ == "__main__":
    main()