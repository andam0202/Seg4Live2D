#!/usr/bin/env python3
"""
SAM2è¶…ç²¾å¯†ãƒ‘ãƒ¼ãƒ„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

å•é¡Œåˆ†æã¨è¶…ç²¾å¯†æ”¹å–„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼š
1. ã‚ˆã‚Šå³å¯†ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¢ƒç•Œè¨­å®š
2. ãƒ‘ãƒ¼ãƒ„ç‰¹æœ‰ã®è§£å‰–å­¦çš„çŸ¥è­˜ã‚’æ´»ç”¨
3. é™¤å¤–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æˆ¦ç•¥çš„å¼·åŒ–
4. ãƒ‘ãƒ¼ãƒ„é–“ã®ç«¶åˆã‚’å®Œå…¨ã«æ’é™¤
"""

import sys
import cv2
import numpy as np
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.utils import setup_logging, get_logger
from src.core.sam2.sam2_model import SAM2ModelManager
from src.core.sam2.prompt_handler import SAM2PromptHandler, PointPrompt

@dataclass
class SegmentationResult:
    """ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœ"""
    part_name: str
    mask: np.ndarray
    score: float
    processing_time: float
    prompts_used: int
    strategy: str

class UltraPreciseSegmenter:
    """è¶…ç²¾å¯†ãƒ‘ãƒ¼ãƒ„ã‚»ã‚°ãƒ¡ãƒ³ã‚¿ãƒ¼"""
    
    def __init__(self, sam2_manager: SAM2ModelManager):
        self.sam2_manager = sam2_manager
        self.logger = get_logger(__name__)
        
    def generate_ultra_precise_prompts(self, image_shape: Tuple[int, int], part_name: str) -> List[PointPrompt]:
        """è¶…ç²¾å¯†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆå•é¡Œè§£æ±ºç‰ˆï¼‰"""
        height, width = image_shape[:2]
        
        # è¶…ç²¾å¯†ã§å¢ƒç•Œã‚’å³æ ¼ã«åŒºåˆ¥ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé…ç½®
        ultra_precise_prompts = {
            "hair": [
                # é«ªã®æ¯›ã®é ˜åŸŸã®ã¿ã«å³å¯†ã«åˆ¶é™
                PointPrompt(width//4, height//6, 1, "å·¦é«ª_å†…å´"),
                PointPrompt(3*width//4, height//6, 1, "å³é«ª_å†…å´"),
                PointPrompt(width//2, height//10, 1, "å¾Œé«ª_ä¸­å¤®"),
                # è§’ã¨é¡”ã‚’å³æ ¼ã«é™¤å¤–
                PointPrompt(width//2, height//20, 0, "è§’_ãƒ„ãƒ_é™¤å¤–"),
                PointPrompt(width//3, height//20, 0, "å·¦è§’_é™¤å¤–"),
                PointPrompt(2*width//3, height//20, 0, "å³è§’_é™¤å¤–"),
                PointPrompt(width//2, height//3, 0, "é¡”_è‚Œ_é™¤å¤–"),
                PointPrompt(width//2, 2*height//5, 0, "é¡_è‚Œ_é™¤å¤–"),
                PointPrompt(width//2, 3*height//5, 0, "ä½“_å®Œå…¨é™¤å¤–"),
            ],
            "face": [
                # é¡”ã®è‚Œã®éƒ¨åˆ†ã®ã¿ã«å³å¯†ã«åˆ¶é™
                PointPrompt(width//2, height//2, 1, "é¡”_ä¸­å¤®_è‚Œ"),
                PointPrompt(2*width//5, height//2, 1, "å·¦é ¬_è‚Œã®ã¿"),
                PointPrompt(3*width//5, height//2, 1, "å³é ¬_è‚Œã®ã¿"),
                PointPrompt(width//2, 3*height//7, 1, "é¡_ä¸‹éƒ¨_è‚Œ"),
                # é«ªã¨ä½“ã‚’å³æ ¼ã«é™¤å¤–
                PointPrompt(width//2, height//4, 0, "é¡ä¸Š_é«ª_é™¤å¤–"),
                PointPrompt(width//2, height//5, 0, "å‰é«ª_å®Œå…¨é™¤å¤–"),
                PointPrompt(width//3, height//4, 0, "å·¦é«ª_å¢ƒç•Œ_é™¤å¤–"),
                PointPrompt(2*width//3, height//4, 0, "å³é«ª_å¢ƒç•Œ_é™¤å¤–"),
                PointPrompt(width//2, 3*height//5, 0, "é¦–_ä½“_é™¤å¤–"),
                PointPrompt(width//6, height//2, 0, "å·¦èƒŒæ™¯_é™¤å¤–"),
                PointPrompt(5*width//6, height//2, 0, "å³èƒŒæ™¯_é™¤å¤–"),
            ],
            "body": [
                # é¦–ã‹ã‚‰ä¸‹ã®ä½“ã®ã¿ã«å³å¯†ã«åˆ¶é™
                PointPrompt(width//2, 2*height//3, 1, "èƒ´ä½“_ä¸­å¤®_ã®ã¿"),
                PointPrompt(2*width//5, 3*height//4, 1, "å·¦è‚©_ä½“ã®ã¿"),
                PointPrompt(3*width//5, 3*height//4, 1, "å³è‚©_ä½“ã®ã¿"),
                # é¡”ã¨é«ªã‚’å³æ ¼ã«é™¤å¤–
                PointPrompt(width//2, height//2, 0, "é¡”_å®Œå…¨é™¤å¤–"),
                PointPrompt(width//2, height//3, 0, "é¦–ä¸Š_é™¤å¤–"),
                PointPrompt(width//2, height//4, 0, "é ­éƒ¨_é™¤å¤–"),
                PointPrompt(width//2, height//6, 0, "é«ª_å®Œå…¨é™¤å¤–"),
                PointPrompt(width//6, 2*height//3, 0, "å·¦èƒŒæ™¯_é™¤å¤–"),
                PointPrompt(5*width//6, 2*height//3, 0, "å³èƒŒæ™¯_é™¤å¤–"),
            ],
            "eyes": [
                # ç›®ã®éƒ¨åˆ†ã®ã¿ã«è¶…å³å¯†ã«åˆ¶é™
                PointPrompt(2*width//5, 2*height//5, 1, "å·¦ç›®_ç³ã®ã¿"),
                PointPrompt(3*width//5, 2*height//5, 1, "å³ç›®_ç³ã®ã¿"),
                # é¡”ã®ä»–ã®éƒ¨åˆ†ã¨é«ªã‚’å³æ ¼ã«é™¤å¤–
                PointPrompt(width//2, height//3, 0, "é¡_é™¤å¤–"),
                PointPrompt(width//2, height//4, 0, "å‰é«ª_é™¤å¤–"),
                PointPrompt(width//2, height//5, 0, "é«ª_å®Œå…¨é™¤å¤–"),
                PointPrompt(width//2, height//2, 0, "é¼»_é™¤å¤–"),
                PointPrompt(width//2, 3*height//5, 0, "å£_é™¤å¤–"),
                PointPrompt(width//4, 2*height//5, 0, "å·¦é ¬_é™¤å¤–"),
                PointPrompt(3*width//4, 2*height//5, 0, "å³é ¬_é™¤å¤–"),
                PointPrompt(width//3, height//3, 0, "å·¦çœ‰_é™¤å¤–"),
                PointPrompt(2*width//3, height//3, 0, "å³çœ‰_é™¤å¤–"),
            ]
        }
        
        prompts = ultra_precise_prompts.get(part_name, ultra_precise_prompts["hair"])
        self.logger.info(f"  ğŸ¯ è¶…ç²¾å¯†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ: {len(prompts)}ç‚¹ ({part_name})")
        return prompts
    
    def segment_part_ultra_precise(self, image_rgb: np.ndarray, part_name: str) -> SegmentationResult:
        """è¶…ç²¾å¯†ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        self.logger.info(f"ğŸ¯ {part_name.upper()}è¶…ç²¾å¯†ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")
        
        start_time = time.time()
        height, width = image_rgb.shape[:2]
        
        # è¶…ç²¾å¯†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompts = self.generate_ultra_precise_prompts((height, width), part_name)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåº§æ¨™ã¨ãƒ©ãƒ™ãƒ«ã‚’æº–å‚™
        point_coords = []
        point_labels = []
        
        for prompt in prompts:
            point_coords.append([prompt.x, prompt.y])
            point_labels.append(prompt.label)
        
        point_coords = np.array(point_coords) if point_coords else None
        point_labels = np.array(point_labels) if point_labels else None
        
        # SAM2ã§è¶…ç²¾å¯†æ¨è«–å®Ÿè¡Œ
        masks, scores, logits = self.sam2_manager.predict(
            image=image_rgb,
            point_coords=point_coords,
            point_labels=point_labels,
            multimask_output=True
        )
        
        if len(masks) == 0:
            raise RuntimeError("ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœãªã—")
        
        # æœ€è‰¯ã®çµæœã‚’é¸æŠ
        best_idx = np.argmax(scores)
        best_mask = masks[best_idx]
        best_score = scores[best_idx]
        
        processing_time = time.time() - start_time
        
        result = SegmentationResult(
            part_name=part_name,
            mask=best_mask,
            score=best_score,
            processing_time=processing_time,
            prompts_used=len(prompts),
            strategy="ultra_precise"
        )
        
        self.logger.info(f"  âœ… å®Œäº†: ã‚¹ã‚³ã‚¢ {best_score:.3f}, æ™‚é–“ {processing_time:.2f}s")
        return result
    
    def create_ultra_detailed_visualization(self, image_rgb: np.ndarray, result: SegmentationResult) -> np.ndarray:
        """è¶…è©³ç´°å¯è¦–åŒ–ï¼ˆå¢ƒç•Œã¨ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’å¼·èª¿ï¼‰"""
        vis_image = image_rgb.copy()
        
        # ãƒ‘ãƒ¼ãƒ„åˆ¥è‰²åˆ†ã‘
        colors = {
            "hair": [255, 100, 150],    # ãƒ”ãƒ³ã‚¯
            "face": [255, 200, 100],    # ã‚ªãƒ¬ãƒ³ã‚¸
            "body": [100, 200, 255],    # é’
            "eyes": [150, 255, 100],    # ç·‘
        }
        color = np.array(colors.get(result.part_name, [255, 255, 255]))
        
        # ãƒã‚¹ã‚¯é©ç”¨ï¼ˆã‚ˆã‚Šæ§ãˆã‚ã«ï¼‰
        mask_bool = result.mask.astype(bool)
        vis_image[mask_bool] = vis_image[mask_bool] * 0.6 + color * 0.4
        
        # ãƒã‚¹ã‚¯å¢ƒç•Œã‚’å¼·èª¿æç”»
        mask_uint8 = (result.mask * 255).astype(np.uint8)
        contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(vis_image, contours, -1, color.tolist(), 3)
        
        # å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹è¨ˆç®—
        if len(contours) > 0:
            x, y, w, h = cv2.boundingRect(contours[0])
            cv2.rectangle(vis_image, (x, y), (x + w, y + h), color.tolist(), 2)
        
        # è¶…è©³ç´°æƒ…å ±è¡¨ç¤º
        coverage = np.sum(result.mask) / result.mask.size * 100
        mask_pixels = int(np.sum(result.mask))
        
        info_lines = [
            f"Part: {result.part_name.upper()}",
            f"Score: {result.score:.3f}",
            f"Strategy: {result.strategy}",
            f"Time: {result.processing_time:.2f}s",
            f"Prompts: {result.prompts_used}",
            f"Mask Pixels: {mask_pixels:,}",
            f"Coverage: {coverage:.2f}%",
            f"Efficiency: {result.score/coverage*100:.1f}" if coverage > 0 else "Efficiency: N/A",
        ]
        
        for i, line in enumerate(info_lines):
            y_pos = 30 + i * 25
            text_size = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(vis_image, (10, y_pos - 20), 
                         (text_size[0] + 20, y_pos + 5), (0, 0, 0), -1)
            cv2.putText(vis_image, line, (15, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return vis_image
    
    def save_ultra_results(self, image_rgb: np.ndarray, results: List[SegmentationResult], 
                          output_dir: Path, image_name: str) -> Dict[str, Any]:
        """è¶…ç²¾å¯†çµæœä¿å­˜"""
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒä¿å­˜
        original_path = output_dir / f"original_{image_name}.png"
        original_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
        cv2.imwrite(str(original_path), original_bgr)
        
        saved_files = {"original": str(original_path)}
        
        # ãƒ‘ãƒ¼ãƒ„åˆ¥çµæœä¿å­˜
        for result in results:
            part_name = result.part_name
            
            # ãƒã‚¹ã‚¯ç”»åƒä¿å­˜ï¼ˆãƒã‚¤ãƒŠãƒªï¼‰
            mask_path = output_dir / f"ultra_mask_{part_name}_{image_name}.png"
            cv2.imwrite(str(mask_path), (result.mask * 255).astype(np.uint8))
            saved_files[f"ultra_mask_{part_name}"] = str(mask_path)
            
            # é€æ˜PNGä¿å­˜ï¼ˆLive2Dç”¨ï¼‰
            transparent_path = output_dir / f"ultra_transparent_{part_name}_{image_name}.png"
            transparent_image = image_rgb.copy()
            alpha_channel = result.mask.astype(np.uint8) * 255
            transparent_rgba = np.dstack([transparent_image, alpha_channel])
            transparent_bgra = cv2.cvtColor(transparent_rgba, cv2.COLOR_RGBA2BGRA)
            cv2.imwrite(str(transparent_path), transparent_bgra)
            saved_files[f"ultra_transparent_{part_name}"] = str(transparent_path)
            
            # è¶…è©³ç´°å¯è¦–åŒ–ç”»åƒä¿å­˜
            vis_image = self.create_ultra_detailed_visualization(image_rgb, result)
            vis_path = output_dir / f"ultra_viz_{part_name}_{image_name}.png"
            vis_bgr = cv2.cvtColor(vis_image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(str(vis_path), vis_bgr)
            saved_files[f"ultra_viz_{part_name}"] = str(vis_path)
            
            # å¢ƒç•Œãƒã‚¹ã‚¯ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            mask_uint8 = (result.mask * 255).astype(np.uint8)
            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            boundary_image = np.zeros_like(image_rgb)
            cv2.drawContours(boundary_image, contours, -1, (255, 255, 255), 2)
            boundary_path = output_dir / f"ultra_boundary_{part_name}_{image_name}.png"
            boundary_bgr = cv2.cvtColor(boundary_image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(str(boundary_path), boundary_bgr)
            saved_files[f"ultra_boundary_{part_name}"] = str(boundary_path)
        
        return saved_files

def parse_args():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ"""
    parser = argparse.ArgumentParser(
        description="SAM2è¶…ç²¾å¯†ãƒ‘ãƒ¼ãƒ„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # è¶…ç²¾å¯†ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
  python scripts/sam2_ultra_precise_segmentation.py --input data/samples/anime_woman1
  
  # ç‰¹å®šãƒ‘ãƒ¼ãƒ„ã®ã¿
  python scripts/sam2_ultra_precise_segmentation.py --parts face --input data/samples/anime_woman1
  
  # è©³ç´°ãƒ­ã‚°ä»˜ã
  python scripts/sam2_ultra_precise_segmentation.py --input data/samples/anime_woman1 --verbose
        """
    )
    
    parser.add_argument(
        "--input", "-i",
        type=str,
        default="data/samples/demo_images",
        help="å…¥åŠ›ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹"
    )
    
    parser.add_argument(
        "--output", "-o", 
        type=str,
        default="data/output/sam2_ultra_precise_segmentation",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹"
    )
    
    parser.add_argument(
        "--parts", "-p",
        nargs="+",
        choices=["face", "hair", "body", "eyes"],
        default=["face", "hair", "body", "eyes"],
        help="ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¯¾è±¡ãƒ‘ãƒ¼ãƒ„"
    )
    
    parser.add_argument(
        "--image-pattern", 
        type=str,
        default="*.png",
        help="ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³"
    )
    
    parser.add_argument(
        "--max-images",
        type=int,
        default=None,
        help="å‡¦ç†ã™ã‚‹æœ€å¤§ç”»åƒæ•°"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º"
    )
    
    return parser.parse_args()

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°è§£æ
    args = parse_args()
    
    # ãƒ­ã‚°è¨­å®š
    log_level = "DEBUG" if args.verbose else "INFO"
    setup_logging(level=log_level, console_output=True, structured=False)
    logger = get_logger(__name__)
    
    logger.info("=== SAM2è¶…ç²¾å¯†ãƒ‘ãƒ¼ãƒ„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ===")
    logger.info(f"ğŸ“ å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€: {args.input}")
    logger.info(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: {args.output}")
    logger.info(f"ğŸ¯ å¯¾è±¡ãƒ‘ãƒ¼ãƒ„: {', '.join(args.parts)}")
    
    try:
        # å…¥åŠ›ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ã®ç¢ºèª
        input_path = Path(args.input)
        if not input_path.is_absolute():
            input_path = project_root / input_path
            
        if not input_path.exists():
            logger.error(f"âŒ å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {input_path}")
            return
            
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
        image_files = list(input_path.glob(args.image_pattern))
        
        if not image_files:
            logger.error(f"âŒ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # ç”»åƒæ•°åˆ¶é™
        if args.max_images and len(image_files) > args.max_images:
            image_files = image_files[:args.max_images]
            
        logger.info(f"ğŸ“¸ å‡¦ç†å¯¾è±¡ç”»åƒæ•°: {len(image_files)}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
        output_path = Path(args.output)
        if not output_path.is_absolute():
            output_path = project_root / output_path
        output_path.mkdir(parents=True, exist_ok=True)
        
        # SAM2ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        logger.info("ğŸ¤– SAM2ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–")
        sam2_manager = SAM2ModelManager(model_name="sam2_hiera_large.pt")
        if not sam2_manager.load_model():
            logger.error("âŒ SAM2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—")
            return
        
        segmenter = UltraPreciseSegmenter(sam2_manager)
        
        # å¯¾è±¡ãƒ‘ãƒ¼ãƒ„
        target_parts = args.parts
        
        # å…¨å‡¦ç†çµ±è¨ˆ
        total_images_processed = 0
        total_processing_time = 0
        all_results = []
        
        # å„ç”»åƒã§å‡¦ç†
        for image_idx, image_path in enumerate(image_files):
            logger.info(f"\nğŸ“¸ ç”»åƒå‡¦ç† ({image_idx + 1}/{len(image_files)}): {image_path.name}")
            
            try:
                # ç”»åƒèª­ã¿è¾¼ã¿
                image = cv2.imread(str(image_path))
                if image is None:
                    logger.warning(f"  âš ï¸ ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {image_path}")
                    continue
                    
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                logger.info(f"  ğŸ“ ç”»åƒã‚µã‚¤ã‚º: {image_rgb.shape[1]}Ã—{image_rgb.shape[0]}")
                
                # å„ãƒ‘ãƒ¼ãƒ„ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
                image_results = []
                image_start_time = time.time()
                
                for part_name in target_parts:
                    logger.info(f"\n  ğŸ¯ {part_name.upper()}ãƒ‘ãƒ¼ãƒ„å‡¦ç†")
                    
                    try:
                        result = segmenter.segment_part_ultra_precise(image_rgb, part_name)
                        image_results.append(result)
                        
                        # å€‹åˆ¥çµæœãƒ­ã‚°
                        coverage = np.sum(result.mask) / result.mask.size * 100
                        efficiency = result.score / coverage * 100 if coverage > 0 else 0
                        logger.info(f"    ğŸ“Š ã‚«ãƒãƒ¼ç‡: {coverage:.2f}%, åŠ¹ç‡æ€§: {efficiency:.1f}")
                        
                    except Exception as e:
                        logger.error(f"    âŒ {part_name}ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—: {e}")
                        continue
                
                image_processing_time = time.time() - image_start_time
                total_processing_time += image_processing_time
                
                if not image_results:
                    logger.warning(f"  âš ï¸ {image_path.name}ã®å‡¦ç†çµæœãŒã‚ã‚Šã¾ã›ã‚“")
                    continue
                
                # è¶…ç²¾å¯†çµæœä¿å­˜
                logger.info(f"\n  ğŸ’¾ è¶…ç²¾å¯†çµæœä¿å­˜ä¸­...")
                saved_files = segmenter.save_ultra_results(image_rgb, image_results, output_path, image_path.stem)
                
                # ç”»åƒåˆ¥ã‚µãƒãƒªãƒ¼
                avg_score = np.mean([r.score for r in image_results])
                avg_coverage = np.mean([np.sum(r.mask) / r.mask.size * 100 for r in image_results])
                
                logger.info(f"  ğŸ“Š {image_path.name} å®Œäº†:")
                logger.info(f"    ğŸ¯ å‡¦ç†ãƒ‘ãƒ¼ãƒ„: {len(image_results)}")
                logger.info(f"    ğŸ“ˆ å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.3f}")
                logger.info(f"    ğŸ“ å¹³å‡ã‚«ãƒãƒ¼ç‡: {avg_coverage:.2f}%")
                logger.info(f"    â±ï¸ å‡¦ç†æ™‚é–“: {image_processing_time:.2f}ç§’")
                
                all_results.extend(image_results)
                total_images_processed += 1
                
            except Exception as e:
                logger.error(f"  âŒ {image_path.name}å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if not all_results:
            logger.warning("âš ï¸ å…¨ä½“ã®å‡¦ç†çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # è¶…ç²¾å¯†åˆ†æçµæœ
        ultra_analysis = {
            "total_images_processed": total_images_processed,
            "total_processing_time": float(total_processing_time),
            "average_score": float(np.mean([r.score for r in all_results])),
            "average_coverage": float(np.mean([np.sum(r.mask) / r.mask.size * 100 for r in all_results])),
            "strategy": "ultra_precise",
            "improvements": {
                "targeted_prompts": "ãƒ‘ãƒ¼ãƒ„ç‰¹æœ‰ã®è§£å‰–å­¦çš„å¢ƒç•Œã‚’å³å¯†ã«è¨­å®š",
                "enhanced_exclusions": "éš£æ¥ãƒ‘ãƒ¼ãƒ„ã®æˆ¦ç•¥çš„é™¤å¤–ã‚’å¼·åŒ–",
                "boundary_precision": "å¢ƒç•Œç·šã®ç²¾åº¦ã‚’å¤§å¹…å‘ä¸Š"
            }
        }
        
        # ãƒ‘ãƒ¼ãƒ„åˆ¥è¶…ç²¾å¯†çµ±è¨ˆ
        parts_stats = {}
        for part in target_parts:
            part_results = [r for r in all_results if r.part_name == part]
            if part_results:
                avg_coverage = np.mean([np.sum(r.mask) / r.mask.size * 100 for r in part_results])
                parts_stats[part] = {
                    "count": len(part_results),
                    "average_score": float(np.mean([r.score for r in part_results])),
                    "average_coverage": float(avg_coverage),
                    "precision": float(np.mean([r.score for r in part_results]) / avg_coverage * 100) if avg_coverage > 0 else 0,
                    "best_score": float(max(part_results, key=lambda r: r.score).score),
                    "most_precise": float(min([np.sum(r.mask) / r.mask.size * 100 for r in part_results]))
                }
        
        ultra_analysis["parts_statistics"] = parts_stats
        
        # è¶…ç²¾å¯†åˆ†æçµæœJSONä¿å­˜
        ultra_analysis_path = output_path / "ultra_precise_analysis.json"
        with open(ultra_analysis_path, 'w', encoding='utf-8') as f:
            json.dump(ultra_analysis, f, indent=2, ensure_ascii=False)
        
        # æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼
        logger.info(f"\nğŸ‰ è¶…ç²¾å¯†å‡¦ç†å®Œäº†!")
        logger.info(f"ğŸ“Š è¶…ç²¾å¯†çµ±è¨ˆ:")
        logger.info(f"  ğŸ“¸ å‡¦ç†ç”»åƒæ•°: {total_images_processed}/{len(image_files)}")
        logger.info(f"  ğŸ¯ ç·ãƒ‘ãƒ¼ãƒ„æ•°: {len(all_results)}")
        logger.info(f"  ğŸ“ˆ å…¨ä½“å¹³å‡ã‚¹ã‚³ã‚¢: {ultra_analysis['average_score']:.3f}")
        logger.info(f"  ğŸ“ å…¨ä½“å¹³å‡ã‚«ãƒãƒ¼ç‡: {ultra_analysis['average_coverage']:.2f}%")
        logger.info(f"  â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_processing_time:.2f}ç§’")
        
        # è¶…ç²¾å¯†ãƒ‘ãƒ¼ãƒ„åˆ¥çµ±è¨ˆè¡¨ç¤º
        logger.info(f"\nğŸ“ è¶…ç²¾å¯†ãƒ‘ãƒ¼ãƒ„åˆ¥çµ±è¨ˆ:")
        for part, stats in parts_stats.items():
            logger.info(f"  {part:6}: {stats['count']:2}ä»¶, "
                       f"å¹³å‡ã‚¹ã‚³ã‚¢ {stats['average_score']:.3f}, "
                       f"å¹³å‡ã‚«ãƒãƒ¼ç‡ {stats['average_coverage']:5.2f}%, "
                       f"ç²¾å¯†åº¦ {stats['precision']:5.1f}")
        
        logger.info(f"\nğŸ“ çµæœä¿å­˜å…ˆ: {output_path}")
        logger.info(f"ğŸ“‹ è¶…ç²¾å¯†åˆ†æ: {ultra_analysis_path}")
        logger.info("âœ¨ è¶…ç²¾å¯†ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è§£æå®Œäº†!")
        
    except Exception as e:
        logger.error(f"âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if 'sam2_manager' in locals():
            sam2_manager.unload_model()

if __name__ == "__main__":
    main()