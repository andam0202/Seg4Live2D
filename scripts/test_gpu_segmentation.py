#!/usr/bin/env python3
"""
GPUä½¿ç”¨çŠ¶æ³ç¢ºèªä»˜ãã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ

GPUãŒæ­£ã—ãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ãªãŒã‚‰ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
"""

import sys
import time
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.utils import setup_logging, get_logger

def check_gpu_status():
    """GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª"""
    import torch
    
    logger = get_logger(__name__)
    
    logger.info("=== GPUçŠ¶æ³ç¢ºèª ===")
    logger.info(f"PyTorch version: {torch.__version__}")
    logger.info(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        logger.info(f"CUDA version: {torch.version.cuda}")
        logger.info(f"Device count: {torch.cuda.device_count()}")
        logger.info(f"Current device: {torch.cuda.current_device()}")
        logger.info(f"Device name: {torch.cuda.get_device_name(0)}")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³
        allocated = torch.cuda.memory_allocated(0) / 1024**2
        reserved = torch.cuda.memory_reserved(0) / 1024**2
        logger.info(f"GPU Memory - Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB")
    else:
        logger.warning("CUDA is not available! Running on CPU.")
    
    return torch.cuda.is_available()

def monitor_gpu_memory(label=""):
    """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ¢ãƒ‹ã‚¿ãƒ¼"""
    import torch
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**2
        reserved = torch.cuda.memory_reserved(0) / 1024**2
        return f"{label} - GPU Memory: Allocated={allocated:.2f}MB, Reserved={reserved:.2f}MB"
    return f"{label} - Running on CPU"

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # ãƒ­ã‚°è¨­å®š
    setup_logging(level="INFO", console_output=True, structured=False)
    logger = get_logger(__name__)
    
    logger.info("=== GPUå¯¾å¿œã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ ===")
    
    # GPUçŠ¶æ³ç¢ºèª
    has_gpu = check_gpu_status()
    
    try:
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨åˆæœŸåŒ–
        from src.core.segmentation import get_segmentation_engine
        from src.core.utils import load_config
        
        # è¨­å®šèª­ã¿è¾¼ã¿ï¼ˆGPUä½¿ç”¨ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰
        config = load_config()
        if has_gpu:
            config.model.device = "cuda"
            logger.info("è¨­å®š: GPU (CUDA) ã‚’ä½¿ç”¨")
        else:
            config.model.device = "cpu"
            logger.info("è¨­å®š: CPU ã‚’ä½¿ç”¨")
        
        # ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        logger.info("\n=== ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ– ===")
        logger.info(monitor_gpu_memory("åˆæœŸåŒ–å‰"))
        
        engine = get_segmentation_engine()
        engine.initialize()
        
        logger.info(monitor_gpu_memory("åˆæœŸåŒ–å¾Œ"))
        
        # ãƒ†ã‚¹ãƒˆç”»åƒã®æº–å‚™
        sample_images_path = project_root / "data" / "samples" / "demo_images"
        image_files = list(sample_images_path.glob("*.png"))[:3]  # æœ€åˆã®3æšã‚’ãƒ†ã‚¹ãƒˆ
        
        if not image_files:
            logger.error("ãƒ†ã‚¹ãƒˆç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        logger.info(f"\nãƒ†ã‚¹ãƒˆç”»åƒ: {len(image_files)}æš")
        
        # å„ç”»åƒã§ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        for i, image_file in enumerate(image_files, 1):
            logger.info(f"\n=== ç”»åƒ {i}/{len(image_files)}: {image_file.name} ===")
            
            logger.info(monitor_gpu_memory("å‡¦ç†å‰"))
            
            start_time = time.time()
            result = engine.process_image(
                image=image_file,
                save_masks=True,
                output_dir=project_root / "data" / "output" / "gpu_test"
            )
            processing_time = time.time() - start_time
            
            logger.info(monitor_gpu_memory("å‡¦ç†å¾Œ"))
            
            if result.success:
                logger.info(f"âœ… æˆåŠŸ - å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
                logger.info(f"   æ¤œå‡ºãƒã‚¹ã‚¯æ•°: {len(result.masks)}")
                logger.info(f"   ãƒ‡ãƒã‚¤ã‚¹: {engine.model_manager.device}")
                
                # æœ€åˆã®5ã¤ã®ãƒã‚¹ã‚¯ã‚’è¡¨ç¤º
                for j, mask in enumerate(result.masks[:5]):
                    logger.info(f"   ãƒã‚¹ã‚¯{j+1}: {mask.class_name} (confidence={mask.confidence:.3f})")
            else:
                logger.error(f"âŒ å¤±æ•—: {result.error_message}")
        
        # æœ€çµ‚ãƒ¡ãƒ¢ãƒªçŠ¶æ³
        logger.info(f"\n{monitor_gpu_memory('æœ€çµ‚çŠ¶æ…‹')}")
        
        # GPUä½¿ç”¨ã®ç·è©•
        if has_gpu and engine.model_manager.device == "cuda":
            logger.info("\nğŸ‰ GPUï¼ˆCUDAï¼‰ã‚’ä½¿ç”¨ã—ã¦ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸï¼")
        else:
            logger.info("\nâš ï¸ CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã¾ã—ãŸ")
        
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if 'engine' in locals() and hasattr(engine, 'cleanup'):
            engine.cleanup()
            logger.info("ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

if __name__ == "__main__":
    main()