#!/usr/bin/env python3
"""
SAM2ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç²¾åº¦å‘ä¸Šãƒ†ã‚¹ãƒˆ

Edge-guided + Iterative + é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’çµ„ã¿åˆã‚ã›ãŸæœ€é«˜ç²¾åº¦æ‰‹æ³•
"""

import sys
import cv2
import numpy as np
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.utils import setup_logging, get_logger
from src.core.sam2.sam2_model import SAM2ModelManager
from src.core.sam2.prompt_handler import SAM2PromptHandler, PointPrompt

@dataclass
class ImageCharacteristics:
    """ç”»åƒç‰¹æ€§åˆ†æçµæœ"""
    brightness: float
    contrast: float
    edge_density: float
    color_variance: float
    is_anime_style: bool

class SAM2HybridEnhancer:
    """SAM2ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç²¾åº¦å‘ä¸Šã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, sam2_manager: SAM2ModelManager):
        self.sam2_manager = sam2_manager
        self.logger = get_logger(__name__)
        
    def analyze_image_characteristics(self, image: np.ndarray) -> ImageCharacteristics:
        """ç”»åƒç‰¹æ€§ã‚’è©³ç´°åˆ†æ"""
        self.logger.info("ğŸ“Š ç”»åƒç‰¹æ€§åˆ†æé–‹å§‹")
        
        # æ˜åº¦åˆ†æ
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        brightness = float(np.mean(gray))
        
        # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆåˆ†æ
        contrast = float(np.std(gray))
        
        # ã‚¨ãƒƒã‚¸å¯†åº¦åˆ†æ
        edges = cv2.Canny(gray, 50, 150)
        edge_density = float(np.sum(edges > 0) / edges.size)
        
        # è‰²åˆ†æ•£åˆ†æ
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        color_variance = float(np.std(hsv[:, :, 1]))  # å½©åº¦ã®åˆ†æ•£
        
        # ã‚¢ãƒ‹ãƒ¡ã‚¹ã‚¿ã‚¤ãƒ«åˆ¤å®šï¼ˆå½©åº¦ãŒé«˜ãã€ã‚¨ãƒƒã‚¸ãŒã¯ã£ãã‚Šã—ã¦ã„ã‚‹ï¼‰
        is_anime_style = color_variance > 30 and edge_density > 0.05
        
        characteristics = ImageCharacteristics(
            brightness=brightness,
            contrast=contrast,
            edge_density=edge_density,
            color_variance=color_variance,
            is_anime_style=is_anime_style
        )
        
        self.logger.info(f"  ğŸ” æ˜åº¦: {brightness:.1f}, ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ: {contrast:.1f}")
        self.logger.info(f"  ğŸŒ ã‚¨ãƒƒã‚¸å¯†åº¦: {edge_density:.3f}, è‰²åˆ†æ•£: {color_variance:.1f}")
        self.logger.info(f"  ğŸ¨ ã‚¢ãƒ‹ãƒ¡ã‚¹ã‚¿ã‚¤ãƒ«: {'Yes' if is_anime_style else 'No'}")
        
        return characteristics
    
    def adaptive_preprocess_image(self, image: np.ndarray, characteristics: ImageCharacteristics) -> np.ndarray:
        """ç”»åƒç‰¹æ€§ã«å¿œã˜ãŸé©å¿œçš„å‰å‡¦ç†"""
        self.logger.info("âš™ï¸ é©å¿œçš„å‰å‡¦ç†é©ç”¨")
        
        processed = image.copy()
        
        # æ˜åº¦ã«å¿œã˜ãŸèª¿æ•´
        if characteristics.brightness < 80:  # æš—ã„ç”»åƒ
            self.logger.info("  ğŸŒ™ æš—ã„ç”»åƒæ¤œå‡º â†’ CLAHEå¼·åŒ–")
            lab = cv2.cvtColor(processed, cv2.COLOR_RGB2LAB)
            l_channel, a, b = cv2.split(lab)
            clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8,8))  # å¼·ã‚ã®CLAHE
            l_channel = clahe.apply(l_channel)
            processed = cv2.merge((l_channel, a, b))
            processed = cv2.cvtColor(processed, cv2.COLOR_LAB2RGB)
            
        elif characteristics.brightness > 180:  # æ˜ã‚‹ã„ç”»åƒ
            self.logger.info("  â˜€ï¸ æ˜ã‚‹ã„ç”»åƒæ¤œå‡º â†’ ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´")
            processed = cv2.convertScaleAbs(processed, alpha=1.2, beta=-20)
        
        # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã«å¿œã˜ãŸèª¿æ•´
        if characteristics.contrast < 30:  # ä½ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ
            self.logger.info("  ğŸ“ˆ ä½ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆæ¤œå‡º â†’ ã‚·ãƒ£ãƒ¼ãƒ—åŒ–")
            kernel = np.array([[-1,-1,-1], [-1, 9,-1], [-1,-1,-1]])
            processed = cv2.filter2D(processed, -1, kernel)
        
        # ã‚¢ãƒ‹ãƒ¡ã‚¹ã‚¿ã‚¤ãƒ«ã«å¿œã˜ãŸèª¿æ•´
        if characteristics.is_anime_style:
            self.logger.info("  ğŸ¨ ã‚¢ãƒ‹ãƒ¡ã‚¹ã‚¿ã‚¤ãƒ«æ¤œå‡º â†’ å½©åº¦å¼·åŒ–")
            hsv = cv2.cvtColor(processed, cv2.COLOR_RGB2HSV)
            hsv[:, :, 1] = cv2.multiply(hsv[:, :, 1], 1.1)  # å½©åº¦ã‚’10%å‘ä¸Š
            processed = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
        
        return processed
    
    def generate_adaptive_edge_prompts(self, image: np.ndarray, base_prompts: List[PointPrompt], 
                                     characteristics: ImageCharacteristics) -> List[PointPrompt]:
        """ç”»åƒç‰¹æ€§ã«å¿œã˜ãŸé©å¿œçš„ã‚¨ãƒƒã‚¸èª˜å°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        self.logger.info("ğŸŒ é©å¿œçš„ã‚¨ãƒƒã‚¸èª˜å°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ")
        
        # ã‚¨ãƒƒã‚¸æ¤œå‡ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”»åƒç‰¹æ€§ã«å¿œã˜ã¦èª¿æ•´
        if characteristics.edge_density > 0.1:  # ã‚¨ãƒƒã‚¸ãŒå¤šã„ç”»åƒ
            low_threshold, high_threshold = 80, 160  # å³ã—ã„é–¾å€¤
            self.logger.info("  ğŸ“Š é«˜ã‚¨ãƒƒã‚¸å¯†åº¦ â†’ å³ã—ã„é–¾å€¤è¨­å®š")
        else:  # ã‚¨ãƒƒã‚¸ãŒå°‘ãªã„ç”»åƒ
            low_threshold, high_threshold = 30, 100  # ç·©ã„é–¾å€¤
            self.logger.info("  ğŸ“Š ä½ã‚¨ãƒƒã‚¸å¯†åº¦ â†’ ç·©ã„é–¾å€¤è¨­å®š")
        
        # Cannyã‚¨ãƒƒã‚¸æ¤œå‡º
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        edges = cv2.Canny(gray, low_threshold, high_threshold)
        
        # å½¢æ…‹å­¦çš„å‡¦ç†ã§ã‚¨ãƒƒã‚¸ã‚’å¼·åŒ–
        kernel = np.ones((3,3), np.uint8)
        edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)
        
        edge_prompts = base_prompts.copy()
        
        # ã‚¨ãƒƒã‚¸ä¸Šã®ç‚¹ã‚’å–å¾—
        edge_points = np.column_stack(np.where(edges > 0))
        
        if len(edge_points) == 0:
            self.logger.warning("  âš ï¸ ã‚¨ãƒƒã‚¸ç‚¹ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return edge_prompts
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°ã‚’ç”»åƒç‰¹æ€§ã«å¿œã˜ã¦èª¿æ•´
        if characteristics.is_anime_style:
            sample_count = min(30, len(edge_points))  # ã‚¢ãƒ‹ãƒ¡ã‚¹ã‚¿ã‚¤ãƒ«ã¯ã‚ˆã‚Šå¤šãã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        else:
            sample_count = min(20, len(edge_points))
        
        # ã‚¨ãƒƒã‚¸ç‚¹ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(edge_points) > sample_count:
            indices = np.random.choice(len(edge_points), sample_count, replace=False)
            sampled_edges = edge_points[indices]
        else:
            sampled_edges = edge_points
        
        # æ—¢å­˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰é ã„ç‚¹ã‚’å„ªå…ˆé¸æŠ
        filtered_edges = []
        for y, x in sampled_edges:
            min_distance = float('inf')
            for base_prompt in base_prompts:
                distance = np.sqrt((x - base_prompt.x)**2 + (y - base_prompt.y)**2)
                min_distance = min(min_distance, distance)
            
            # è·é›¢ãŒ30ãƒ”ã‚¯ã‚»ãƒ«ä»¥ä¸Šé›¢ã‚Œã¦ã„ã‚‹ç‚¹ã®ã¿è¿½åŠ 
            if min_distance > 30:
                filtered_edges.append((y, x))
        
        # ã‚¨ãƒƒã‚¸ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        for y, x in filtered_edges:
            edge_prompts.append(
                PointPrompt(int(x), int(y), 1, description="é©å¿œçš„ã‚¨ãƒƒã‚¸èª˜å°ç‚¹")
            )
        
        self.logger.info(f"  ğŸ¯ ã‚¨ãƒƒã‚¸ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ : +{len(filtered_edges)}ç‚¹ (ç·è¨ˆ{len(edge_prompts)}ç‚¹)")
        return edge_prompts
    
    def advanced_iterative_refinement(self, image: np.ndarray, initial_prompts: List[PointPrompt], 
                                    characteristics: ImageCharacteristics, 
                                    max_iterations: int = 4) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[PointPrompt]]:
        """é«˜åº¦ãªåå¾©çš„ãƒã‚¹ã‚¯æ”¹è‰¯"""
        self.logger.info(f"ğŸ”„ é«˜åº¦ãªåå¾©çš„æ”¹è‰¯é–‹å§‹ (æœ€å¤§{max_iterations}å›)")
        
        current_prompts = initial_prompts.copy()
        best_score = -1
        best_result = None
        convergence_threshold = 0.005  # åæŸåˆ¤å®šé–¾å€¤
        
        for iteration in range(max_iterations):
            self.logger.info(f"  ğŸ”„ åå¾© {iteration + 1}/{max_iterations}")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
            handler = SAM2PromptHandler()
            handler.start_new_session()
            
            for prompt in current_prompts:
                handler.add_point_prompt(
                    prompt.x, prompt.y, prompt.label, prompt.description
                )
            
            # æ¨è«–å®Ÿè¡Œ
            sam2_prompts = handler.get_sam2_prompts()
            masks, scores, logits = self.sam2_manager.predict(
                image=image,
                **sam2_prompts,
                multimask_output=True
            )
            
            current_best_score = float(np.max(scores))
            current_best_mask = masks[np.argmax(scores)]
            
            self.logger.info(f"    ğŸ“Š ã‚¹ã‚³ã‚¢: {current_best_score:.3f}")
            
            # æœ€é«˜ã‚¹ã‚³ã‚¢æ›´æ–°
            score_improvement = current_best_score - best_score
            if score_improvement > 0:
                best_score = current_best_score
                best_result = (masks, scores, logits)
                self.logger.info(f"    âœ¨ æ–°æœ€é«˜ã‚¹ã‚³ã‚¢! æ”¹å–„: +{score_improvement:.3f}")
            
            # åæŸåˆ¤å®š
            if iteration > 0 and score_improvement < convergence_threshold:
                self.logger.info(f"    ğŸ¯ åæŸæ¤œå‡º (æ”¹å–„: {score_improvement:.3f} < {convergence_threshold:.3f})")
                break
            
            # æ¬¡ã®åå¾©ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹è‰¯
            if iteration < max_iterations - 1:
                self.logger.info("    ğŸ”§ æ¬¡å›åå¾©ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹è‰¯")
                
                # ãƒã‚¹ã‚¯ã‚¨ãƒƒã‚¸ã®è©³ç´°åˆ†æ
                mask_uint8 = (current_best_mask * 255).astype(np.uint8)
                
                # å¤–å´ã‚¨ãƒƒã‚¸ï¼ˆå½é™½æ€§ã‚’æ¸›ã‚‰ã™è² ä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
                dilated = cv2.dilate(mask_uint8, np.ones((5,5), np.uint8), iterations=1)
                outer_edge = dilated - mask_uint8
                outer_points = np.column_stack(np.where(outer_edge > 0))
                
                # å†…å´ã‚¨ãƒƒã‚¸ï¼ˆå½é™°æ€§ã‚’æ¸›ã‚‰ã™æ­£ä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
                eroded = cv2.erode(mask_uint8, np.ones((3,3), np.uint8), iterations=1)
                inner_edge = mask_uint8 - eroded
                inner_points = np.column_stack(np.where(inner_edge > 0))
                
                # è² ä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ ï¼ˆå¤–å´ã‚¨ãƒƒã‚¸ã‹ã‚‰ï¼‰
                if len(outer_points) > 0:
                    n_negative = min(3, len(outer_points))  # æœ€å¤§3ç‚¹
                    neg_indices = np.random.choice(len(outer_points), n_negative, replace=False)
                    for idx in neg_indices:
                        y, x = outer_points[idx]
                        current_prompts.append(
                            PointPrompt(int(x), int(y), 0, f"åå¾©æ”¹è‰¯_è² ä¾‹_{iteration}")
                        )
                    self.logger.info(f"      â– è² ä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ : {n_negative}ç‚¹")
                
                # æ­£ä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ ï¼ˆå†…å´ã‚¨ãƒƒã‚¸ã‹ã‚‰ï¼‰
                if len(inner_points) > 0:
                    n_positive = min(2, len(inner_points))  # æœ€å¤§2ç‚¹
                    pos_indices = np.random.choice(len(inner_points), n_positive, replace=False)
                    for idx in pos_indices:
                        y, x = inner_points[idx]
                        current_prompts.append(
                            PointPrompt(int(x), int(y), 1, f"åå¾©æ”¹è‰¯_æ­£ä¾‹_{iteration}")
                        )
                    self.logger.info(f"      â• æ­£ä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ : {n_positive}ç‚¹")
        
        self.logger.info(f"  ğŸ åå¾©æ”¹è‰¯å®Œäº†: æœ€çµ‚ã‚¹ã‚³ã‚¢ {best_score:.3f}")
        return best_result[0], best_result[1], best_result[2], current_prompts
    
    def hybrid_enhancement(self, image: np.ndarray, base_prompts: List[PointPrompt], 
                         part_name: str) -> Tuple[np.ndarray, float, Dict[str, Any]]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç²¾åº¦å‘ä¸Šæ‰‹æ³•ã®çµ±åˆå®Ÿè¡Œ"""
        self.logger.info(f"ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç²¾åº¦å‘ä¸Šé–‹å§‹ - ãƒ‘ãƒ¼ãƒ„: {part_name}")
        
        start_time = time.time()
        enhancement_log = {
            "part_name": part_name,
            "phases": [],
            "total_prompts": len(base_prompts)
        }
        
        # Phase 1: ç”»åƒç‰¹æ€§åˆ†æ
        self.logger.info("ğŸ“‹ Phase 1: ç”»åƒç‰¹æ€§åˆ†æ")
        characteristics = self.analyze_image_characteristics(image)
        enhancement_log["characteristics"] = characteristics.__dict__
        
        # Phase 2: é©å¿œçš„å‰å‡¦ç†
        self.logger.info("ğŸ“‹ Phase 2: é©å¿œçš„å‰å‡¦ç†")
        processed_image = self.adaptive_preprocess_image(image, characteristics)
        enhancement_log["phases"].append("adaptive_preprocessing")
        
        # Phase 3: é©å¿œçš„ã‚¨ãƒƒã‚¸èª˜å°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        self.logger.info("ğŸ“‹ Phase 3: é©å¿œçš„ã‚¨ãƒƒã‚¸èª˜å°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ")
        edge_enhanced_prompts = self.generate_adaptive_edge_prompts(
            processed_image, base_prompts, characteristics
        )
        enhancement_log["edge_prompts_added"] = len(edge_enhanced_prompts) - len(base_prompts)
        enhancement_log["phases"].append("adaptive_edge_guided")
        
        # Phase 4: é«˜åº¦ãªåå¾©çš„æ”¹è‰¯
        self.logger.info("ğŸ“‹ Phase 4: é«˜åº¦ãªåå¾©çš„æ”¹è‰¯")
        final_masks, final_scores, final_logits, final_prompts = self.advanced_iterative_refinement(
            processed_image, edge_enhanced_prompts, characteristics
        )
        enhancement_log["final_prompts_count"] = len(final_prompts)
        enhancement_log["phases"].append("advanced_iterative")
        
        # æœ€çµ‚çµæœ
        best_mask = final_masks[np.argmax(final_scores)]
        best_score = float(np.max(final_scores))
        
        total_time = time.time() - start_time
        enhancement_log["processing_time"] = total_time
        enhancement_log["final_score"] = best_score
        
        self.logger.info(f"ğŸ‰ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç²¾åº¦å‘ä¸Šå®Œäº†!")
        self.logger.info(f"  ğŸ“Š æœ€çµ‚ã‚¹ã‚³ã‚¢: {best_score:.3f}")
        self.logger.info(f"  â±ï¸ å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
        self.logger.info(f"  ğŸ¯ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {len(base_prompts)} â†’ {len(final_prompts)}")
        
        return best_mask, best_score, enhancement_log

def test_hybrid_enhancement(
    image_rgb: np.ndarray,
    part_name: str, 
    base_prompts: List[PointPrompt],
    enhancer: SAM2HybridEnhancer,
    output_dir: Path,
    image_name: str
) -> Dict[str, Any]:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç²¾åº¦å‘ä¸Šæ‰‹æ³•ã®ãƒ†ã‚¹ãƒˆ"""
    logger = get_logger(__name__)
    
    try:
        logger.info(f"ğŸ§ª ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ãƒ†ã‚¹ãƒˆé–‹å§‹ - {part_name}")
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•å®Ÿè¡Œ
        best_mask, best_score, enhancement_log = enhancer.hybrid_enhancement(
            image_rgb, base_prompts, part_name
        )
        
        # çµæœå¯è¦–åŒ–
        overlay = image_rgb.copy().astype(np.float32)
        color = np.array([255, 100, 100])  # èµ¤ç³»
        mask_bool = best_mask.astype(bool)  # ãƒ–ãƒ¼ãƒ«å‹ã«å¤‰æ›
        overlay[mask_bool] = overlay[mask_bool] * 0.6 + color * 0.4
        overlay_image = overlay.astype(np.uint8)
        
        # æƒ…å ±è¡¨ç¤º
        info_lines = [
            f"Hybrid Enhancement: {part_name}",
            f"Score: {best_score:.3f}",
            f"Time: {enhancement_log['processing_time']:.2f}s",
            f"Prompts: {enhancement_log['total_prompts']} -> {enhancement_log['final_prompts_count']}",
            f"Phases: {len(enhancement_log['phases'])}",
        ]
        
        for i, line in enumerate(info_lines):
            cv2.putText(overlay_image, line, (10, 30 + i * 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # ä¿å­˜
        output_path = output_dir / f"hybrid_{part_name}_{image_name}.png"
        overlay_bgr = cv2.cvtColor(overlay_image, cv2.COLOR_RGB2BGR)
        cv2.imwrite(str(output_path), overlay_bgr)
        
        # è©³ç´°ãƒ­ã‚°ä¿å­˜
        log_path = output_dir / f"hybrid_{part_name}_{image_name}_log.txt"
        with open(log_path, 'w', encoding='utf-8') as f:
            f.write(f"=== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç²¾åº¦å‘ä¸Šãƒ­ã‚° ===\n")
            f.write(f"ãƒ‘ãƒ¼ãƒ„: {part_name}\n")
            f.write(f"æœ€çµ‚ã‚¹ã‚³ã‚¢: {best_score:.3f}\n")
            f.write(f"å‡¦ç†æ™‚é–“: {enhancement_log['processing_time']:.2f}ç§’\n")
            f.write(f"å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º: {', '.join(enhancement_log['phases'])}\n")
            f.write(f"\n=== ç”»åƒç‰¹æ€§ ===\n")
            for key, value in enhancement_log['characteristics'].items():
                f.write(f"{key}: {value}\n")
        
        logger.info(f"âœ… ãƒ†ã‚¹ãƒˆå®Œäº† - ã‚¹ã‚³ã‚¢: {best_score:.3f}")
        
        return {
            "technique": "hybrid_enhancement",
            "part_name": part_name,
            "score": best_score,
            "inference_time": enhancement_log['processing_time'],
            "enhancement_log": enhancement_log,
            "output_path": str(output_path),
            "log_path": str(log_path)
        }
        
    except Exception as e:
        logger.error(f"âŒ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ãƒ†ã‚¹ãƒˆå¤±æ•— {part_name}: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # ãƒ­ã‚°è¨­å®š
    setup_logging(level="INFO", console_output=True, structured=False)
    logger = get_logger(__name__)
    
    logger.info("=== SAM2ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç²¾åº¦å‘ä¸Šãƒ†ã‚¹ãƒˆ ===")
    
    try:
        # æœ¬ç•ªç”»åƒå–å¾—
        sample_images_path = project_root / "data" / "samples" / "demo_images2"
        image_files = list(sample_images_path.glob("*.png"))
        
        if not image_files:
            logger.error("âŒ æœ¬ç•ªç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (data/samples/demo_images2/)")
            return
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
        output_dir = project_root / "data" / "output" / "sam2_hybrid_enhancement"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # SAM2ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        logger.info("ğŸ¤– SAM2ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–")
        sam2_manager = SAM2ModelManager(model_name="sam2_hiera_large.pt")
        if not sam2_manager.load_model():
            logger.error("âŒ SAM2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—")
            return
        
        enhancer = SAM2HybridEnhancer(sam2_manager)
        
        # æœ¬ç•ªç”»åƒã§ãƒ†ã‚¹ãƒˆ
        image_path = image_files[0]
        logger.info(f"ğŸ“¸ æœ¬ç•ªç”»åƒ: {image_path.name}")
        
        # ç”»åƒèª­ã¿è¾¼ã¿
        image = cv2.imread(str(image_path))
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        height, width = image_rgb.shape[:2]
        
        # ãƒ†ã‚¹ãƒˆå¯¾è±¡ï¼šé«ªã®æ¯›ãƒ‘ãƒ¼ãƒ„
        from scripts.test_sam2_detailed_parts import MainPartPrompts
        hair_prompts = MainPartPrompts.hair((height, width))
        
        logger.info(f"ğŸ¯ ãƒ†ã‚¹ãƒˆå¯¾è±¡: é«ªã®æ¯› ({len(hair_prompts)}å€‹ã®åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ)")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼ˆæ¯”è¼ƒç”¨ï¼‰
        logger.info("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šé–‹å§‹")
        baseline_handler = SAM2PromptHandler()
        baseline_handler.start_new_session()
        
        for prompt in hair_prompts:
            baseline_handler.add_point_prompt(
                prompt.x, prompt.y, prompt.label, prompt.description
            )
        
        baseline_start = time.time()
        sam2_prompts = baseline_handler.get_sam2_prompts()
        masks, scores, logits = sam2_manager.predict(
            image=image_rgb,
            **sam2_prompts,
            multimask_output=True
        )
        baseline_time = time.time() - baseline_start
        baseline_score = float(np.max(scores))
        
        logger.info(f"ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœ: ã‚¹ã‚³ã‚¢ {baseline_score:.3f}, æ™‚é–“ {baseline_time:.2f}ç§’")
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•å®Ÿè¡Œ
        logger.info("ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•å®Ÿè¡Œé–‹å§‹")
        result = test_hybrid_enhancement(
            image_rgb, "hair", hair_prompts, 
            enhancer, output_dir, image_path.stem
        )
        
        if "error" not in result:
            # çµæœæ¯”è¼ƒ
            hybrid_score = result["score"]
            hybrid_time = result["inference_time"]
            improvement = ((hybrid_score - baseline_score) / baseline_score) * 100
            
            logger.info(f"\nğŸ† æœ€çµ‚çµæœæ¯”è¼ƒ:")
            logger.info(f"  ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:     ã‚¹ã‚³ã‚¢ {baseline_score:.3f}, æ™‚é–“ {baseline_time:.2f}ç§’")
            logger.info(f"  ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰:     ã‚¹ã‚³ã‚¢ {hybrid_score:.3f}, æ™‚é–“ {hybrid_time:.2f}ç§’")
            logger.info(f"  ğŸ“ˆ ç²¾åº¦å‘ä¸Š:         +{improvement:.1f}%")
            logger.info(f"  â±ï¸ å‡¦ç†æ™‚é–“æ¯”:       {hybrid_time/baseline_time:.1f}å€")
            
            if hybrid_score > 0.90:
                logger.info("ğŸ‰ ç›®æ¨™ã‚¹ã‚³ã‚¢0.90ã‚’é”æˆ!")
            elif improvement > 5.0:
                logger.info("âœ¨ 5%ä»¥ä¸Šã®ç²¾åº¦å‘ä¸Šã‚’é”æˆ!")
            
            logger.info(f"\nğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}")
            logger.info(f"  ğŸ–¼ï¸ çµæœç”»åƒ: {result['output_path']}")
            logger.info(f"  ğŸ“‹ è©³ç´°ãƒ­ã‚°: {result['log_path']}")
        
        logger.info("ğŸ‰ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç²¾åº¦å‘ä¸Šãƒ†ã‚¹ãƒˆå®Œäº†!")
        
    except Exception as e:
        logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if 'sam2_manager' in locals():
            sam2_manager.unload_model()

if __name__ == "__main__":
    main()