#!/usr/bin/env python3
"""
SAM2ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ†ã‚¹ãƒˆ

å…¨SAM2ãƒ¢ãƒ‡ãƒ«ï¼ˆtiny/small/base+/largeï¼‰ã§ã®ç²¾åº¦ãƒ»é€Ÿåº¦æ¯”è¼ƒã‚’è¤‡æ•°ç”»åƒã§å®Ÿè¡Œ
"""

import sys
import cv2
import numpy as np
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.utils import setup_logging, get_logger
from src.core.sam2.sam2_model import SAM2ModelManager
from src.core.sam2.prompt_handler import SAM2PromptHandler, Live2DPromptPresets

def test_single_model_on_image(
    model_name: str,
    image_path: Path,
    image_rgb: np.ndarray,
    test_prompts: Dict[str, List]
) -> Dict[str, Any]:
    """å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§ã®å˜ä¸€ç”»åƒãƒ†ã‚¹ãƒˆ"""
    logger = get_logger(__name__)
    
    try:
        # SAM2ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        start_time = time.time()
        sam2_manager = SAM2ModelManager(model_name=model_name)
        if not sam2_manager.load_model():
            return {"error": f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {model_name}"}
        
        model_load_time = time.time() - start_time
        
        results = {
            "model_name": model_name,
            "image_name": image_path.stem,
            "model_load_time": model_load_time,
            "part_results": {}
        }
        
        # å„éƒ¨ä½ã§ãƒ†ã‚¹ãƒˆ
        for part_name, prompts in test_prompts.items():
            logger.info(f"  {part_name} ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ - {model_name}")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
            handler = SAM2PromptHandler()
            handler.start_new_session()
            
            for prompt in prompts:
                handler.add_point_prompt(
                    prompt.x, prompt.y, prompt.label,
                    description=prompt.description
                )
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            start_time = time.time()
            sam2_prompts = handler.get_sam2_prompts()
            masks, scores, logits = sam2_manager.predict(
                image=image_rgb,
                **sam2_prompts,
                multimask_output=True
            )
            inference_time = time.time() - start_time
            
            # çµæœè¨˜éŒ²
            best_score = float(np.max(scores))
            best_mask = masks[np.argmax(scores)].astype(bool)
            mask_coverage = float(np.sum(best_mask) / best_mask.size)
            
            results["part_results"][part_name] = {
                "score": best_score,
                "inference_time": inference_time,
                "mask_coverage": mask_coverage,
                "num_masks": len(masks)
            }
            
            logger.info(f"    ã‚¹ã‚³ã‚¢: {best_score:.3f}, æ™‚é–“: {inference_time:.2f}s")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        sam2_manager.unload_model()
        
        return results
        
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆå¤±æ•— {model_name}: {e}")
        return {"error": str(e)}

def create_comparison_visualization(
    image_rgb: np.ndarray,
    all_results: List[Dict[str, Any]],
    output_path: Path
) -> None:
    """æ¯”è¼ƒçµæœã®å¯è¦–åŒ–ç”»åƒã‚’ä½œæˆ"""
    logger = get_logger(__name__)
    
    try:
        models = ["sam2_hiera_tiny.pt", "sam2_hiera_small.pt", "sam2_hiera_base_plus.pt", "sam2_hiera_large.pt"]
        model_short_names = ["Tiny", "Small", "Base+", "Large"]
        
        # 2x2ã‚°ãƒªãƒƒãƒ‰ã§çµæœè¡¨ç¤º
        height, width = image_rgb.shape[:2]
        grid_image = np.zeros((height * 2, width * 2, 3), dtype=np.uint8)
        
        positions = [(0, 0), (0, 1), (1, 0), (1, 1)]
        
        for idx, (model_name, short_name, pos) in enumerate(zip(models, model_short_names, positions)):
            row, col = pos
            y_start, y_end = row * height, (row + 1) * height
            x_start, x_end = col * width, (col + 1) * width
            
            # ãƒ™ãƒ¼ã‚¹ç”»åƒã‚’ã‚³ãƒ”ãƒ¼
            model_image = image_rgb.copy()
            
            # å¯¾å¿œã™ã‚‹çµæœã‚’æ¤œç´¢
            model_result = None
            for result in all_results:
                if result.get("model_name") == model_name and "error" not in result:
                    model_result = result
                    break
            
            if model_result:
                # ã‚¹ã‚³ã‚¢æƒ…å ±ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤
                y_offset = 30
                cv2.putText(model_image, f"{short_name}", (10, y_offset), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
                
                y_offset += 40
                for part_name, part_result in model_result["part_results"].items():
                    score_text = f"{part_name}: {part_result['score']:.3f}"
                    cv2.putText(model_image, score_text, (10, y_offset), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                    y_offset += 25
                
                # æ¨è«–æ™‚é–“è¡¨ç¤º
                total_inference_time = sum(p["inference_time"] for p in model_result["part_results"].values())
                time_text = f"Time: {total_inference_time:.2f}s"
                cv2.putText(model_image, time_text, (10, height - 20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            else:
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                cv2.putText(model_image, f"{short_name} - ERROR", (10, height//2), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
            
            # ã‚°ãƒªãƒƒãƒ‰ã«é…ç½®
            grid_image[y_start:y_end, x_start:x_end] = model_image
        
        # ä¿å­˜
        grid_bgr = cv2.cvtColor(grid_image, cv2.COLOR_RGB2BGR)
        cv2.imwrite(str(output_path), grid_bgr)
        logger.info(f"æ¯”è¼ƒå¯è¦–åŒ–ä¿å­˜: {output_path}")
        
    except Exception as e:
        logger.error(f"å¯è¦–åŒ–ä½œæˆå¤±æ•—: {e}")

def create_detailed_comparison_report(
    all_results: List[Dict[str, Any]],
    output_path: Path
) -> None:
    """è©³ç´°æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’JSONå½¢å¼ã§ä¿å­˜"""
    logger = get_logger(__name__)
    
    try:
        # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
        summary = {
            "models": ["sam2_hiera_tiny.pt", "sam2_hiera_small.pt", "sam2_hiera_base_plus.pt", "sam2_hiera_large.pt"],
            "parts": ["hair", "face", "body"],
            "detailed_results": all_results,
            "summary_stats": {}
        }
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¥çµ±è¨ˆ
        for model_name in summary["models"]:
            model_results = [r for r in all_results if r.get("model_name") == model_name and "error" not in r]
            
            if model_results:
                model_stats = {
                    "avg_scores": {},
                    "avg_inference_time": {},
                    "total_time": {}
                }
                
                for part_name in summary["parts"]:
                    scores = [r["part_results"][part_name]["score"] for r in model_results if part_name in r["part_results"]]
                    times = [r["part_results"][part_name]["inference_time"] for r in model_results if part_name in r["part_results"]]
                    
                    if scores:
                        model_stats["avg_scores"][part_name] = {
                            "mean": float(np.mean(scores)),
                            "std": float(np.std(scores)),
                            "min": float(np.min(scores)),
                            "max": float(np.max(scores))
                        }
                    
                    if times:
                        model_stats["avg_inference_time"][part_name] = {
                            "mean": float(np.mean(times)),
                            "std": float(np.std(times))
                        }
                
                # å…¨ä½“æ™‚é–“çµ±è¨ˆ
                total_times = []
                for result in model_results:
                    total_time = sum(p["inference_time"] for p in result["part_results"].values())
                    total_times.append(total_time)
                
                if total_times:
                    model_stats["total_time"] = {
                        "mean": float(np.mean(total_times)),
                        "std": float(np.std(total_times))
                    }
                
                summary["summary_stats"][model_name] = model_stats
        
        # JSONä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")
        
    except Exception as e:
        logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆä½œæˆå¤±æ•—: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # ãƒ­ã‚°è¨­å®š
    setup_logging(level="INFO", console_output=True, structured=False)
    logger = get_logger(__name__)
    
    logger.info("=== SAM2ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ†ã‚¹ãƒˆ ===")
    
    try:
        # ãƒ†ã‚¹ãƒˆç”»åƒå–å¾—
        sample_images_path = project_root / "data" / "samples" / "demo_images"
        image_files = list(sample_images_path.glob("*.png"))
        
        if not image_files:
            logger.error("âŒ ãƒ†ã‚¹ãƒˆç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
        output_dir = project_root / "data" / "output" / "sam2_model_comparison"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ†ã‚¹ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        models_to_test = [
            "sam2_hiera_tiny.pt",
            "sam2_hiera_small.pt", 
            "sam2_hiera_base_plus.pt",
            "sam2_hiera_large.pt"
        ]
        
        all_results = []
        
        # å„ç”»åƒã§ãƒ†ã‚¹ãƒˆ
        for img_idx, image_path in enumerate(image_files[:5]):  # æœ€å¤§5æšã§ãƒ†ã‚¹ãƒˆ
            logger.info(f"\\nğŸ“¸ ç”»åƒ {img_idx + 1}/{min(5, len(image_files))}: {image_path.name}")
            
            # ç”»åƒèª­ã¿è¾¼ã¿
            image = cv2.imread(str(image_path))
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            height, width = image_rgb.shape[:2]
            
            logger.info(f"   ç”»åƒã‚µã‚¤ã‚º: {width}x{height}")
            
            # Live2Dãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™
            test_prompts = {
                "hair": Live2DPromptPresets.hair_segmentation((height, width)),
                "face": Live2DPromptPresets.face_segmentation((height, width)),
                "body": Live2DPromptPresets.body_segmentation((height, width))
            }
            
            image_results = []
            
            # å„ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
            for model_name in models_to_test:
                logger.info(f"  ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model_name}")
                
                result = test_single_model_on_image(
                    model_name, image_path, image_rgb, test_prompts
                )
                
                if "error" not in result:
                    image_results.append(result)
                    all_results.append(result)
                    
                    # å„éƒ¨ä½ã®çµæœè¡¨ç¤º
                    for part_name, part_result in result["part_results"].items():
                        logger.info(f"    {part_name}: ã‚¹ã‚³ã‚¢ {part_result['score']:.3f}, "
                                  f"æ™‚é–“ {part_result['inference_time']:.2f}s")
                else:
                    logger.error(f"    âŒ {result['error']}")
            
            # ç”»åƒåˆ¥æ¯”è¼ƒå¯è¦–åŒ–
            if image_results:
                comparison_image_path = output_dir / f"comparison_{image_path.stem}.png"
                create_comparison_visualization(image_rgb, image_results, comparison_image_path)
        
        # å…¨ä½“çµæœãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        if all_results:
            logger.info(f"\\nğŸ“Š å…¨ä½“çµæœã‚µãƒãƒªãƒ¼ä½œæˆä¸­...")
            
            # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
            report_path = output_dir / "detailed_comparison_report.json"
            create_detailed_comparison_report(all_results, report_path)
            
            # ç°¡æ˜“ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            logger.info(f"\\nğŸ† ãƒ¢ãƒ‡ãƒ«åˆ¥å¹³å‡ã‚¹ã‚³ã‚¢:")
            
            models = ["sam2_hiera_tiny.pt", "sam2_hiera_small.pt", "sam2_hiera_base_plus.pt", "sam2_hiera_large.pt"]
            parts = ["hair", "face", "body"]
            
            for model_name in models:
                model_results = [r for r in all_results if r.get("model_name") == model_name and "error" not in r]
                
                if model_results:
                    logger.info(f"  {model_name}:")
                    
                    for part_name in parts:
                        scores = [r["part_results"][part_name]["score"] for r in model_results if part_name in r["part_results"]]
                        if scores:
                            avg_score = np.mean(scores)
                            logger.info(f"    {part_name}: {avg_score:.3f} (Â±{np.std(scores):.3f})")
                    
                    # å¹³å‡æ¨è«–æ™‚é–“
                    total_times = []
                    for result in model_results:
                        total_time = sum(p["inference_time"] for p in result["part_results"].values())
                        total_times.append(total_time)
                    
                    if total_times:
                        avg_time = np.mean(total_times)
                        logger.info(f"    å¹³å‡æ¨è«–æ™‚é–“: {avg_time:.2f}s")
        
        logger.info(f"\\nğŸ‰ ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Œäº†!")
        logger.info(f"çµæœ: {output_dir}")
        logger.info(f"  - æ¯”è¼ƒç”»åƒ: comparison_*.png")
        logger.info(f"  - è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: detailed_comparison_report.json")
        
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()