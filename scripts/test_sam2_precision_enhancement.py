#!/usr/bin/env python3
"""
SAM2ç²¾åº¦å‘ä¸Šãƒ†ã‚¹ãƒˆ

æ§˜ã€…ãªæ‰‹æ³•ã§SAM2ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ãƒ†ã‚¹ãƒˆ
"""

import sys
import cv2
import numpy as np
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.utils import setup_logging, get_logger
from src.core.sam2.sam2_model import SAM2ModelManager
from src.core.sam2.prompt_handler import SAM2PromptHandler, PointPrompt

@dataclass
class EnhancementTechnique:
    """ç²¾åº¦å‘ä¸Šæ‰‹æ³•"""
    name: str
    description: str
    enabled: bool = True

class SAM2PrecisionEnhancer:
    """SAM2ç²¾åº¦å‘ä¸Šã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, sam2_manager: SAM2ModelManager):
        self.sam2_manager = sam2_manager
        self.logger = get_logger(__name__)
    
    def preprocess_image_contrast(self, image: np.ndarray, alpha: float = 1.5, beta: int = 20) -> np.ndarray:
        """ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãƒ»æ˜åº¦èª¿æ•´ã«ã‚ˆã‚‹å‰å‡¦ç†"""
        enhanced = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)
        self.logger.info(f"  ğŸ“ˆ ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´: Î±={alpha}, Î²={beta}")
        return enhanced
    
    def preprocess_image_sharpen(self, image: np.ndarray) -> np.ndarray:
        """ã‚·ãƒ£ãƒ¼ãƒ—åŒ–ã«ã‚ˆã‚‹å‰å‡¦ç†"""
        kernel = np.array([[-1,-1,-1],
                          [-1, 9,-1],
                          [-1,-1,-1]])
        sharpened = cv2.filter2D(image, -1, kernel)
        self.logger.info(f"  ğŸ” ã‚·ãƒ£ãƒ¼ãƒ—åŒ–é©ç”¨")
        return sharpened
    
    def preprocess_image_denoise(self, image: np.ndarray) -> np.ndarray:
        """ãƒã‚¤ã‚ºé™¤å»ã«ã‚ˆã‚‹å‰å‡¦ç†"""
        denoised = cv2.bilateralFilter(image, 9, 75, 75)
        self.logger.info(f"  ğŸ§¹ ãƒã‚¤ã‚ºé™¤å»é©ç”¨")
        return denoised
    
    def preprocess_image_clahe(self, image: np.ndarray) -> np.ndarray:
        """CLAHEï¼ˆé©å¿œçš„ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å‡ç­‰åŒ–ï¼‰ã«ã‚ˆã‚‹å‰å‡¦ç†"""
        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
        l_channel, a, b = cv2.split(lab)
        
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        l_channel = clahe.apply(l_channel)
        
        enhanced = cv2.merge((l_channel, a, b))
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2RGB)
        
        self.logger.info(f"  âš¡ CLAHEé©ç”¨")
        return enhanced
    
    def generate_dense_prompts(self, part_name: str, image_shape: Tuple[int, int], 
                             base_prompts: List[PointPrompt], density: int = 3) -> List[PointPrompt]:
        """å¯†é›†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        height, width = image_shape[:2]
        
        dense_prompts = base_prompts.copy()
        
        # æ—¢å­˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‘¨è¾ºã«è¿½åŠ ç‚¹ã‚’ç”Ÿæˆ
        for base_prompt in base_prompts:
            if base_prompt.label == 1:  # å‰æ™¯ç‚¹ã®ã¿
                for i in range(density):
                    # åŠå¾„å†…ã«ãƒ©ãƒ³ãƒ€ãƒ ç‚¹ã‚’è¿½åŠ 
                    radius = 20 + i * 10
                    angle = (2 * np.pi * i) / density
                    
                    new_x = int(base_prompt.x + radius * np.cos(angle))
                    new_y = int(base_prompt.y + radius * np.sin(angle))
                    
                    # ç”»åƒå¢ƒç•Œå†…ãƒã‚§ãƒƒã‚¯
                    if 0 <= new_x < width and 0 <= new_y < height:
                        dense_prompts.append(
                            PointPrompt(new_x, new_y, 1, 
                                      description=f"{base_prompt.description}_å¯†é›†{i+1}")
                        )
        
        self.logger.info(f"  ğŸ¯ å¯†é›†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ: {len(base_prompts)} â†’ {len(dense_prompts)}ç‚¹")
        return dense_prompts
    
    def generate_edge_guided_prompts(self, image: np.ndarray, 
                                   base_prompts: List[PointPrompt]) -> List[PointPrompt]:
        """ã‚¨ãƒƒã‚¸èª˜å°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        # Cannyã‚¨ãƒƒã‚¸æ¤œå‡º
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        
        edge_prompts = base_prompts.copy()
        
        # ã‚¨ãƒƒã‚¸ä¸Šã®ç‚¹ã‚’è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦ä½¿ç”¨
        edge_points = np.column_stack(np.where(edges > 0))
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå…¨ã‚¨ãƒƒã‚¸ç‚¹ã¯å¤šã™ãã‚‹ãŸã‚ï¼‰
        if len(edge_points) > 20:
            indices = np.random.choice(len(edge_points), 20, replace=False)
            sampled_edges = edge_points[indices]
        else:
            sampled_edges = edge_points
        
        for y, x in sampled_edges:
            edge_prompts.append(
                PointPrompt(int(x), int(y), 1, description="ã‚¨ãƒƒã‚¸èª˜å°ç‚¹")
            )
        
        self.logger.info(f"  ğŸŒ ã‚¨ãƒƒã‚¸èª˜å°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ : +{len(sampled_edges)}ç‚¹")
        return edge_prompts
    
    def multi_scale_inference(self, image: np.ndarray, prompts: List[PointPrompt], 
                            scales: List[float] = [0.8, 1.0, 1.2]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¨è«–"""
        height, width = image.shape[:2]
        best_score = -1
        best_masks = None
        best_scores = None
        best_logits = None
        
        for scale in scales:
            self.logger.info(f"  ğŸ“ ã‚¹ã‚±ãƒ¼ãƒ« {scale} ã§æ¨è«–å®Ÿè¡Œ")
            
            # ç”»åƒãƒªã‚µã‚¤ã‚º
            new_width = int(width * scale)
            new_height = int(height * scale)
            scaled_image = cv2.resize(image, (new_width, new_height))
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåº§æ¨™ã‚‚ã‚¹ã‚±ãƒ¼ãƒ«
            scaled_prompts = []
            for prompt in prompts:
                scaled_x = int(prompt.x * scale)
                scaled_y = int(prompt.y * scale)
                scaled_prompts.append(
                    PointPrompt(scaled_x, scaled_y, prompt.label, prompt.description)
                )
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
            handler = SAM2PromptHandler()
            handler.start_new_session()
            
            for prompt in scaled_prompts:
                handler.add_point_prompt(
                    prompt.x, prompt.y, prompt.label, prompt.description
                )
            
            # æ¨è«–å®Ÿè¡Œ
            sam2_prompts = handler.get_sam2_prompts()
            masks, scores, logits = self.sam2_manager.predict(
                image=scaled_image,
                **sam2_prompts,
                multimask_output=True
            )
            
            # çµæœã‚’å…ƒã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
            resized_masks = []
            for mask in masks:
                resized_mask = cv2.resize(mask.astype(np.uint8), (width, height))
                resized_masks.append(resized_mask.astype(bool))
            
            # æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’æ›´æ–°
            current_best_score = np.max(scores)
            if current_best_score > best_score:
                best_score = current_best_score
                best_masks = np.array(resized_masks)
                best_scores = scores
                best_logits = logits
                self.logger.info(f"    âœ¨ æ–°æœ€é«˜ã‚¹ã‚³ã‚¢: {current_best_score:.3f}")
        
        return best_masks, best_scores, best_logits
    
    def iterative_refinement(self, image: np.ndarray, initial_prompts: List[PointPrompt], 
                           max_iterations: int = 3) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[PointPrompt]]:
        """åå¾©çš„ãƒã‚¹ã‚¯æ”¹è‰¯"""
        current_prompts = initial_prompts.copy()
        best_score = -1
        best_result = None
        
        for iteration in range(max_iterations):
            self.logger.info(f"  ğŸ”„ åå¾© {iteration + 1}/{max_iterations}")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
            handler = SAM2PromptHandler()
            handler.start_new_session()
            
            for prompt in current_prompts:
                handler.add_point_prompt(
                    prompt.x, prompt.y, prompt.label, prompt.description
                )
            
            # æ¨è«–å®Ÿè¡Œ
            sam2_prompts = handler.get_sam2_prompts()
            masks, scores, logits = self.sam2_manager.predict(
                image=image,
                **sam2_prompts,
                multimask_output=True
            )
            
            current_best_score = np.max(scores)
            current_best_mask = masks[np.argmax(scores)]
            
            self.logger.info(f"    ã‚¹ã‚³ã‚¢: {current_best_score:.3f}")
            
            if current_best_score > best_score:
                best_score = current_best_score
                best_result = (masks, scores, logits)
            
            # æ¬¡ã®åå¾©ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹è‰¯
            if iteration < max_iterations - 1:
                # ãƒã‚¹ã‚¯ã‚¨ãƒƒã‚¸ä»˜è¿‘ã«è² ä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
                edges = cv2.Canny((current_best_mask * 255).astype(np.uint8), 50, 150)
                edge_points = np.column_stack(np.where(edges > 0))
                
                if len(edge_points) > 5:
                    # ãƒ©ãƒ³ãƒ€ãƒ ã«5ç‚¹é¸æŠã—ã¦è² ä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦è¿½åŠ 
                    indices = np.random.choice(len(edge_points), 5, replace=False)
                    for idx in indices:
                        y, x = edge_points[idx]
                        # ãƒã‚¹ã‚¯å¤–å´ã®ç‚¹ã‚’è² ä¾‹ã¨ã—ã¦è¿½åŠ 
                        if not current_best_mask[y, x]:
                            current_prompts.append(
                                PointPrompt(int(x), int(y), 0, f"åå¾©æ”¹è‰¯_è² ä¾‹_{iteration}")
                            )
        
        return best_result[0], best_result[1], best_result[2], current_prompts
    
    def ensemble_prediction(self, image: np.ndarray, prompts: List[PointPrompt], 
                          techniques: List[str] = ['normal', 'contrast', 'sharpen']) -> Tuple[np.ndarray, float]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬"""
        self.logger.info(f"  ğŸ­ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬: {techniques}")
        
        all_masks = []
        all_scores = []
        
        for technique in techniques:
            processed_image = image.copy()
            
            # å‰å‡¦ç†é©ç”¨
            if technique == 'contrast':
                processed_image = self.preprocess_image_contrast(processed_image)
            elif technique == 'sharpen':
                processed_image = self.preprocess_image_sharpen(processed_image)
            elif technique == 'denoise':
                processed_image = self.preprocess_image_denoise(processed_image)
            elif technique == 'clahe':
                processed_image = self.preprocess_image_clahe(processed_image)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
            handler = SAM2PromptHandler()
            handler.start_new_session()
            
            for prompt in prompts:
                handler.add_point_prompt(
                    prompt.x, prompt.y, prompt.label, prompt.description
                )
            
            # æ¨è«–å®Ÿè¡Œ
            sam2_prompts = handler.get_sam2_prompts()
            masks, scores, logits = self.sam2_manager.predict(
                image=processed_image,
                **sam2_prompts,
                multimask_output=True
            )
            
            best_idx = np.argmax(scores)
            all_masks.append(masks[best_idx])
            all_scores.append(scores[best_idx])
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆï¼ˆå¤šæ•°æ±ºï¼‰
        ensemble_mask = np.mean(all_masks, axis=0) > 0.5
        ensemble_score = np.mean(all_scores)
        
        self.logger.info(f"    ğŸ† ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚¹ã‚³ã‚¢: {ensemble_score:.3f}")
        
        return ensemble_mask, ensemble_score

def test_enhancement_technique(
    image_rgb: np.ndarray,
    part_name: str, 
    base_prompts: List[PointPrompt],
    technique: EnhancementTechnique,
    enhancer: SAM2PrecisionEnhancer,
    output_dir: Path,
    image_name: str
) -> Dict[str, Any]:
    """å€‹åˆ¥ã®ç²¾åº¦å‘ä¸Šæ‰‹æ³•ã‚’ãƒ†ã‚¹ãƒˆ"""
    logger = get_logger(__name__)
    
    try:
        logger.info(f"  ğŸ§ª æ‰‹æ³•ãƒ†ã‚¹ãƒˆ: {technique.name}")
        
        start_time = time.time()
        
        if technique.name == "baseline":
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆé€šå¸¸ã®SAM2ï¼‰
            handler = SAM2PromptHandler()
            handler.start_new_session()
            
            for prompt in base_prompts:
                handler.add_point_prompt(
                    prompt.x, prompt.y, prompt.label, prompt.description
                )
            
            sam2_prompts = handler.get_sam2_prompts()
            masks, scores, logits = enhancer.sam2_manager.predict(
                image=image_rgb,
                **sam2_prompts,
                multimask_output=True
            )
            
            best_mask = masks[np.argmax(scores)].astype(bool)
            best_score = float(np.max(scores))
        
        elif technique.name == "dense_prompts":
            # å¯†é›†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            dense_prompts = enhancer.generate_dense_prompts(
                part_name, image_rgb.shape[:2], base_prompts, density=4
            )
            
            handler = SAM2PromptHandler()
            handler.start_new_session()
            
            for prompt in dense_prompts:
                handler.add_point_prompt(
                    prompt.x, prompt.y, prompt.label, prompt.description
                )
            
            sam2_prompts = handler.get_sam2_prompts()
            masks, scores, logits = enhancer.sam2_manager.predict(
                image=image_rgb,
                **sam2_prompts,
                multimask_output=True
            )
            
            best_mask = masks[np.argmax(scores)].astype(bool)
            best_score = float(np.max(scores))
        
        elif technique.name == "edge_guided":
            # ã‚¨ãƒƒã‚¸èª˜å°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            edge_prompts = enhancer.generate_edge_guided_prompts(image_rgb, base_prompts)
            
            handler = SAM2PromptHandler()
            handler.start_new_session()
            
            for prompt in edge_prompts:
                handler.add_point_prompt(
                    prompt.x, prompt.y, prompt.label, prompt.description
                )
            
            sam2_prompts = handler.get_sam2_prompts()
            masks, scores, logits = enhancer.sam2_manager.predict(
                image=image_rgb,
                **sam2_prompts,
                multimask_output=True
            )
            
            best_mask = masks[np.argmax(scores)].astype(bool)
            best_score = float(np.max(scores))
        
        elif technique.name == "multi_scale":
            # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¨è«–
            masks, scores, logits = enhancer.multi_scale_inference(
                image_rgb, base_prompts, scales=[0.8, 1.0, 1.2, 1.4]
            )
            
            best_mask = masks[np.argmax(scores)].astype(bool)
            best_score = float(np.max(scores))
        
        elif technique.name == "iterative":
            # åå¾©çš„æ”¹è‰¯
            masks, scores, logits, final_prompts = enhancer.iterative_refinement(
                image_rgb, base_prompts, max_iterations=3
            )
            
            best_mask = masks[np.argmax(scores)].astype(bool)
            best_score = float(np.max(scores))
        
        elif technique.name == "ensemble":
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
            best_mask, best_score = enhancer.ensemble_prediction(
                image_rgb, base_prompts, 
                techniques=['normal', 'contrast', 'sharpen', 'clahe']
            )
        
        else:
            # å‰å‡¦ç†ãƒ™ãƒ¼ã‚¹æ‰‹æ³•
            processed_image = image_rgb.copy()
            
            if technique.name == "contrast_enhanced":
                processed_image = enhancer.preprocess_image_contrast(processed_image)
            elif technique.name == "sharpened":
                processed_image = enhancer.preprocess_image_sharpen(processed_image)
            elif technique.name == "denoised":
                processed_image = enhancer.preprocess_image_denoise(processed_image)
            elif technique.name == "clahe":
                processed_image = enhancer.preprocess_image_clahe(processed_image)
            
            handler = SAM2PromptHandler()
            handler.start_new_session()
            
            for prompt in base_prompts:
                handler.add_point_prompt(
                    prompt.x, prompt.y, prompt.label, prompt.description
                )
            
            sam2_prompts = handler.get_sam2_prompts()
            masks, scores, logits = enhancer.sam2_manager.predict(
                image=processed_image,
                **sam2_prompts,
                multimask_output=True
            )
            
            best_mask = masks[np.argmax(scores)].astype(bool)
            best_score = float(np.max(scores))
        
        inference_time = time.time() - start_time
        mask_coverage = float(np.sum(best_mask) / best_mask.size)
        
        # çµæœå¯è¦–åŒ–
        overlay = image_rgb.copy().astype(np.float32)
        color = np.array([255, 100, 100])  # èµ¤ç³»
        overlay[best_mask] = overlay[best_mask] * 0.6 + color * 0.4
        overlay_image = overlay.astype(np.uint8)
        
        # æƒ…å ±è¡¨ç¤º
        info_lines = [
            f"Technique: {technique.name}",
            f"Score: {best_score:.3f}",
            f"Time: {inference_time:.2f}s",
            f"Coverage: {mask_coverage*100:.1f}%",
        ]
        
        for i, line in enumerate(info_lines):
            cv2.putText(overlay_image, line, (10, 30 + i * 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # ä¿å­˜
        output_path = output_dir / f"{technique.name}_{part_name}_{image_name}.png"
        overlay_bgr = cv2.cvtColor(overlay_image, cv2.COLOR_RGB2BGR)
        cv2.imwrite(str(output_path), overlay_bgr)
        
        logger.info(f"    ğŸ“Š çµæœ: ã‚¹ã‚³ã‚¢ {best_score:.3f}, æ™‚é–“ {inference_time:.2f}s")
        
        return {
            "technique": technique.name,
            "part_name": part_name,
            "score": best_score,
            "inference_time": inference_time,
            "mask_coverage": mask_coverage,
            "output_path": str(output_path)
        }
        
    except Exception as e:
        logger.error(f"æ‰‹æ³•ãƒ†ã‚¹ãƒˆå¤±æ•— {technique.name}: {e}")
        return {"error": str(e)}

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # ãƒ­ã‚°è¨­å®š
    setup_logging(level="INFO", console_output=True, structured=False)
    logger = get_logger(__name__)
    
    logger.info("=== SAM2ç²¾åº¦å‘ä¸Šãƒ†ã‚¹ãƒˆ ===")
    
    try:
        # æœ¬ç•ªç”»åƒå–å¾—
        sample_images_path = project_root / "data" / "samples" / "demo_images2"
        image_files = list(sample_images_path.glob("*.png"))
        
        if not image_files:
            logger.error("âŒ æœ¬ç•ªç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (data/samples/demo_images2/)")
            return
        
        if len(image_files) > 1:
            logger.warning(f"è¤‡æ•°ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚æœ€åˆã®ç”»åƒã‚’ä½¿ç”¨: {image_files[0].name}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
        output_dir = project_root / "data" / "output" / "sam2_precision_enhancement"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # SAM2ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        sam2_manager = SAM2ModelManager(model_name="sam2_hiera_large.pt")
        if not sam2_manager.load_model():
            logger.error("âŒ SAM2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—")
            return
        
        enhancer = SAM2PrecisionEnhancer(sam2_manager)
        
        # ç²¾åº¦å‘ä¸Šæ‰‹æ³•å®šç¾©
        enhancement_techniques = [
            EnhancementTechnique("baseline", "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆé€šå¸¸ã®SAM2ï¼‰"),
            EnhancementTechnique("contrast_enhanced", "ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–"),
            EnhancementTechnique("sharpened", "ã‚·ãƒ£ãƒ¼ãƒ—åŒ–"),
            EnhancementTechnique("denoised", "ãƒã‚¤ã‚ºé™¤å»"),
            EnhancementTechnique("clahe", "é©å¿œçš„ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å‡ç­‰åŒ–"),
            EnhancementTechnique("dense_prompts", "å¯†é›†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"),
            EnhancementTechnique("edge_guided", "ã‚¨ãƒƒã‚¸èª˜å°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"),
            EnhancementTechnique("multi_scale", "ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¨è«–"),
            EnhancementTechnique("iterative", "åå¾©çš„ãƒã‚¹ã‚¯æ”¹è‰¯"),
            EnhancementTechnique("ensemble", "ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬"),
        ]
        
        # æœ¬ç•ªç”»åƒã§ãƒ†ã‚¹ãƒˆ
        image_path = image_files[0]
        logger.info(f"ğŸ“¸ æœ¬ç•ªç”»åƒ: {image_path.name}")
        
        # ç”»åƒèª­ã¿è¾¼ã¿
        image = cv2.imread(str(image_path))
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        height, width = image_rgb.shape[:2]
        
        # ãƒ†ã‚¹ãƒˆå¯¾è±¡ï¼šé«ªã®æ¯›ãƒ‘ãƒ¼ãƒ„
        from scripts.test_sam2_detailed_parts import MainPartPrompts
        hair_prompts = MainPartPrompts.hair((height, width))
        
        all_results = []
        
        # å„æ‰‹æ³•ã‚’ãƒ†ã‚¹ãƒˆ
        for technique in enhancement_techniques:
            logger.info(f"\\nğŸ”¬ ç²¾åº¦å‘ä¸Šæ‰‹æ³•: {technique.name}")
            logger.info(f"   {technique.description}")
            
            result = test_enhancement_technique(
                image_rgb, "hair", hair_prompts, technique, 
                enhancer, output_dir, image_path.stem
            )
            
            if "error" not in result:
                all_results.append(result)
        
        # çµæœåˆ†æ
        logger.info(f"\\nğŸ“Š ç²¾åº¦å‘ä¸Šçµæœåˆ†æ:")
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(all_results, key=lambda x: x["score"], reverse=True)
        
        baseline_score = None
        for result in sorted_results:
            if result["technique"] == "baseline":
                baseline_score = result["score"]
                break
        
        logger.info(f"\\nğŸ† æ‰‹æ³•åˆ¥æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
        for i, result in enumerate(sorted_results):
            technique = result["technique"]
            score = result["score"]
            time_taken = result["inference_time"]
            
            improvement = ""
            if baseline_score and technique != "baseline":
                improvement_pct = ((score - baseline_score) / baseline_score) * 100
                improvement = f" ({improvement_pct:+.1f}%)"
            
            logger.info(f"  {i+1:2d}. {technique:20} - ã‚¹ã‚³ã‚¢: {score:.3f}{improvement}, æ™‚é–“: {time_taken:.2f}s")
        
        # æœ€é«˜æ€§èƒ½æ‰‹æ³•
        best_technique = sorted_results[0]
        logger.info(f"\\nğŸ¥‡ æœ€é«˜æ€§èƒ½æ‰‹æ³•: {best_technique['technique']}")
        logger.info(f"   ã‚¹ã‚³ã‚¢: {best_technique['score']:.3f}")
        logger.info(f"   å‡¦ç†æ™‚é–“: {best_technique['inference_time']:.2f}s")
        
        if baseline_score:
            improvement = ((best_technique['score'] - baseline_score) / baseline_score) * 100
            logger.info(f"   ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”æ”¹å–„: +{improvement:.1f}%")
        
        logger.info(f"\\nğŸ‰ ç²¾åº¦å‘ä¸Šãƒ†ã‚¹ãƒˆå®Œäº†!")
        logger.info(f"çµæœ: {output_dir}")
        
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if 'sam2_manager' in locals():
            sam2_manager.unload_model()

if __name__ == "__main__":
    main()