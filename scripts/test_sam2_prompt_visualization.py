#!/usr/bin/env python3
"""
SAM2ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯è¦–åŒ–ãƒ†ã‚¹ãƒˆ

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‚¹ã‚’å¯è¦–åŒ–ã—ã¦ã€å„æˆ¦ç•¥ã®é•ã„ã‚’æ˜ç¢ºã«è¡¨ç¤º
"""

import sys
import cv2
import numpy as np
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.utils import setup_logging, get_logger
from src.core.sam2.sam2_model import SAM2ModelManager
from src.core.sam2.prompt_handler import SAM2PromptHandler, PointPrompt

# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from scripts.test_sam2_hybrid_enhancement import SAM2HybridEnhancer
from scripts.test_sam2_comprehensive_prompts import ComprehensivePromptTester

class PromptVisualizer:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯è¦–åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
    def draw_prompts_on_image(self, image: np.ndarray, prompts: List[PointPrompt], 
                            title: str = "") -> np.ndarray:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‚¹ã‚’ç”»åƒä¸Šã«æç”»"""
        
        # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼
        vis_image = image.copy()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‚¹ã‚’æç”»
        for i, prompt in enumerate(prompts):
            x, y = int(prompt.x), int(prompt.y)
            
            # æ­£ä¾‹ï¼ˆç·‘ï¼‰ã¨è² ä¾‹ï¼ˆèµ¤ï¼‰ã§è‰²åˆ†ã‘
            if prompt.label == 1:  # æ­£ä¾‹
                color = (0, 255, 0)  # ç·‘
                marker = "+"
            else:  # è² ä¾‹
                color = (255, 0, 0)  # èµ¤  
                marker = "-"
            
            # å††ã‚’æç”»
            cv2.circle(vis_image, (x, y), 8, color, -1)  # å¡—ã‚Šã¤ã¶ã—å††
            cv2.circle(vis_image, (x, y), 12, (255, 255, 255), 2)  # ç™½ã„ç¸
            
            # ç•ªå·ã¨èª¬æ˜ã‚’æç”»
            font_scale = 0.4
            font_thickness = 1
            
            # ç•ªå·
            number_text = f"{i+1}"
            number_size = cv2.getTextSize(number_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)[0]
            number_x = x - number_size[0] // 2
            number_y = y + number_size[1] // 2
            
            # èª¬æ˜æ–‡
            description = prompt.description if prompt.description else f"Point_{i+1}"
            desc_size = cv2.getTextSize(description, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)[0]
            
            # èª¬æ˜æ–‡ã®ä½ç½®ï¼ˆç‚¹ã®ä¸‹å´ã«é…ç½®ï¼‰
            desc_x = x - desc_size[0] // 2
            desc_y = y + 25 + desc_size[1]
            
            # ç•ªå·ã®èƒŒæ™¯ï¼ˆå††ã®ä¸­å¤®ï¼‰
            cv2.rectangle(vis_image, 
                         (number_x - 2, number_y - number_size[1] - 2), 
                         (number_x + number_size[0] + 2, number_y + 2), 
                         (0, 0, 0), -1)
            cv2.putText(vis_image, number_text, (number_x, number_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)
            
            # èª¬æ˜æ–‡ã®èƒŒæ™¯
            padding = 3
            cv2.rectangle(vis_image, 
                         (desc_x - padding, desc_y - desc_size[1] - padding), 
                         (desc_x + desc_size[0] + padding, desc_y + padding), 
                         (0, 0, 0), -1)  # é»’ã„èƒŒæ™¯
            
            # èª¬æ˜æ–‡ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ­£ä¾‹ã¯ç·‘ã€è² ä¾‹ã¯èµ¤ï¼‰
            text_color = (0, 255, 0) if prompt.label == 1 else (255, 0, 0)
            cv2.putText(vis_image, description, (desc_x, desc_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, font_thickness)
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æç”»
        if title:
            title_lines = [
                f"Strategy: {title}",
                f"Prompts: {len(prompts)} points",
                f"Positive: {sum(1 for p in prompts if p.label == 1)}",
                f"Negative: {sum(1 for p in prompts if p.label == 0)}"
            ]
            
            for i, line in enumerate(title_lines):
                y_pos = 30 + i * 25
                # é»’ã„èƒŒæ™¯
                text_size = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                cv2.rectangle(vis_image, (10, y_pos - 20), (text_size[0] + 20, y_pos + 5), (0, 0, 0), -1)
                # ç™½ã„æ–‡å­—
                cv2.putText(vis_image, line, (15, y_pos), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return vis_image
    
    def create_comparison_image(self, image: np.ndarray, 
                              prompt_strategies: Dict[str, List[PointPrompt]],
                              part_name: str) -> np.ndarray:
        """è¤‡æ•°æˆ¦ç•¥ã®æ¯”è¼ƒç”»åƒã‚’ä½œæˆ"""
        
        strategy_names = list(prompt_strategies.keys())
        num_strategies = len(strategy_names)
        
        if num_strategies == 0:
            return image
        
        # 2Ã—3ã¾ãŸã¯3Ã—2ã®ã‚°ãƒªãƒƒãƒ‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
        if num_strategies <= 3:
            rows, cols = 1, num_strategies
        else:
            rows, cols = 2, (num_strategies + 1) // 2
        
        # å„ç”»åƒã®ã‚µã‚¤ã‚ºã‚’èª¿æ•´
        img_height, img_width = image.shape[:2]
        cell_width = img_width // cols
        cell_height = img_height // rows
        
        # æ¯”è¼ƒç”»åƒä½œæˆ
        comparison_image = np.zeros((rows * cell_height, cols * cell_width, 3), dtype=np.uint8)
        
        for i, strategy_name in enumerate(strategy_names):
            prompts = prompt_strategies[strategy_name]
            
            # ä½ç½®è¨ˆç®—
            row = i // cols
            col = i % cols
            
            # ç”»åƒãƒªã‚µã‚¤ã‚º
            resized_image = cv2.resize(image, (cell_width, cell_height))
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåº§æ¨™ã‚‚ãƒªã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦èª¿æ•´
            scale_x = cell_width / img_width
            scale_y = cell_height / img_height
            
            scaled_prompts = []
            for prompt in prompts:
                scaled_prompt = PointPrompt(
                    int(prompt.x * scale_x),
                    int(prompt.y * scale_y),
                    prompt.label,
                    prompt.description
                )
                scaled_prompts.append(scaled_prompt)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æç”»
            vis_image = self.draw_prompts_on_image(resized_image, scaled_prompts, strategy_name)
            
            # æ¯”è¼ƒç”»åƒã«é…ç½®
            y_start = row * cell_height
            y_end = y_start + cell_height
            x_start = col * cell_width
            x_end = x_start + cell_width
            
            comparison_image[y_start:y_end, x_start:x_end] = vis_image
        
        # å…¨ä½“ã‚¿ã‚¤ãƒˆãƒ«
        title_height = 50
        final_image = np.zeros((comparison_image.shape[0] + title_height, comparison_image.shape[1], 3), dtype=np.uint8)
        final_image[title_height:, :] = comparison_image
        
        # ã‚¿ã‚¤ãƒˆãƒ«æç”»
        title_text = f"Prompt Strategies Comparison - {part_name.upper()} Part"
        text_size = cv2.getTextSize(title_text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)[0]
        text_x = (final_image.shape[1] - text_size[0]) // 2
        text_y = 35
        
        cv2.putText(final_image, title_text, (text_x, text_y), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
        
        return final_image

def test_prompt_visualization(
    image_rgb: np.ndarray,
    part_name: str,
    tester: ComprehensivePromptTester,
    output_dir: Path,
    image_name: str
) -> Dict[str, Any]:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯è¦–åŒ–ãƒ†ã‚¹ãƒˆ"""
    logger = get_logger(__name__)
    visualizer = PromptVisualizer()
    
    logger.info(f"ğŸ¯ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯è¦–åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹ - {part_name}")
    
    height, width = image_rgb.shape[:2]
    
    # å„æˆ¦ç•¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
    strategies_prompts = {
        "basic": tester.generate_basic_prompts((height, width), part_name),
        "dense_grid": tester.generate_dense_grid_prompts((height, width), part_name),
        "anatomical": tester.generate_anatomical_prompts((height, width), part_name),
        "semantic": tester.generate_semantic_prompts((height, width), part_name),
        "adaptive_sparse": tester.generate_adaptive_sparse_prompts((height, width), part_name),
    }
    
    # å„æˆ¦ç•¥ã®å€‹åˆ¥å¯è¦–åŒ–
    individual_images = {}
    for strategy_name, prompts in strategies_prompts.items():
        logger.info(f"  ğŸ¨ {strategy_name}æˆ¦ç•¥å¯è¦–åŒ–: {len(prompts)}ç‚¹")
        
        vis_image = visualizer.draw_prompts_on_image(image_rgb, prompts, strategy_name)
        individual_images[strategy_name] = vis_image
        
        # å€‹åˆ¥ç”»åƒä¿å­˜
        individual_path = output_dir / f"prompts_{strategy_name}_{part_name}_{image_name}.png"
        individual_bgr = cv2.cvtColor(vis_image, cv2.COLOR_RGB2BGR)
        cv2.imwrite(str(individual_path), individual_bgr)
    
    # æ¯”è¼ƒç”»åƒä½œæˆ
    logger.info(f"  ğŸ“Š æ¯”è¼ƒç”»åƒä½œæˆ")
    comparison_image = visualizer.create_comparison_image(image_rgb, strategies_prompts, part_name)
    
    # æ¯”è¼ƒç”»åƒä¿å­˜
    comparison_path = output_dir / f"prompts_comparison_{part_name}_{image_name}.png"
    comparison_bgr = cv2.cvtColor(comparison_image, cv2.COLOR_RGB2BGR)
    cv2.imwrite(str(comparison_path), comparison_bgr)
    
    # è©³ç´°æƒ…å ±ä¿å­˜
    prompt_info = {}
    for strategy_name, prompts in strategies_prompts.items():
        prompt_info[strategy_name] = {
            "total_prompts": len(prompts),
            "positive_prompts": sum(1 for p in prompts if p.label == 1),
            "negative_prompts": sum(1 for p in prompts if p.label == 0),
            "points": [
                {
                    "x": int(p.x), "y": int(p.y), 
                    "label": p.label, "description": p.description
                } for p in prompts
            ]
        }
    
    info_path = output_dir / f"prompts_info_{part_name}_{image_name}.json"
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(prompt_info, f, indent=2, ensure_ascii=False)
    
    logger.info(f"  âœ… å¯è¦–åŒ–å®Œäº†")
    logger.info(f"    ğŸ“ æ¯”è¼ƒç”»åƒ: {comparison_path}")
    logger.info(f"    ğŸ“‹ è©³ç´°æƒ…å ±: {info_path}")
    
    return {
        "part_name": part_name,
        "strategies": list(strategies_prompts.keys()),
        "comparison_image_path": str(comparison_path),
        "individual_images": {k: str(output_dir / f"prompts_{k}_{part_name}_{image_name}.png") 
                            for k in strategies_prompts.keys()},
        "prompt_info_path": str(info_path),
        "total_strategies": len(strategies_prompts)
    }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # ãƒ­ã‚°è¨­å®š
    setup_logging(level="INFO", console_output=True, structured=False)
    logger = get_logger(__name__)
    
    logger.info("=== SAM2ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯è¦–åŒ–ãƒ†ã‚¹ãƒˆ ===")
    
    try:
        # æœ¬ç•ªç”»åƒå–å¾—
        sample_images_path = project_root / "data" / "samples" / "demo_images2"
        image_files = list(sample_images_path.glob("*.png"))
        
        if not image_files:
            logger.error("âŒ æœ¬ç•ªç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (data/samples/demo_images2/)")
            return
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
        output_dir = project_root / "data" / "output" / "sam2_prompt_visualization"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # SAM2ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã®ã¿ãªã®ã§ãƒ­ãƒ¼ãƒ‰ã¯ä¸è¦ï¼‰
        logger.info("ğŸ¨ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        sam2_manager = SAM2ModelManager(model_name="sam2_hiera_large.pt")
        tester = ComprehensivePromptTester(sam2_manager)
        
        # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ‘ãƒ¼ãƒ„
        test_parts = ["hair", "face", "body", "eyes"]
        
        # æœ¬ç•ªç”»åƒã§ãƒ†ã‚¹ãƒˆ
        image_path = image_files[0]
        logger.info(f"ğŸ“¸ æœ¬ç•ªç”»åƒ: {image_path.name}")
        
        # ç”»åƒèª­ã¿è¾¼ã¿
        image = cv2.imread(str(image_path))
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        all_results = {}
        
        # å„ãƒ‘ãƒ¼ãƒ„ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯è¦–åŒ–
        for part_name in test_parts:
            logger.info(f"\nğŸ¯ {part_name.upper()}ãƒ‘ãƒ¼ãƒ„ - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯è¦–åŒ–")
            
            result = test_prompt_visualization(
                image_rgb, part_name, tester, output_dir, image_path.stem
            )
            
            all_results[part_name] = result
        
        # å…¨ä½“ã‚µãƒãƒªãƒ¼
        logger.info(f"\nğŸ“Š å¯è¦–åŒ–çµæœã‚µãƒãƒªãƒ¼:")
        
        total_images = 0
        for part_name, result in all_results.items():
            strategies_count = result["total_strategies"]
            total_images += strategies_count + 1  # å€‹åˆ¥ç”»åƒ + æ¯”è¼ƒç”»åƒ
            
            logger.info(f"  ğŸ¨ {part_name.upper()}: {strategies_count}æˆ¦ç•¥")
            logger.info(f"    ğŸ“ æ¯”è¼ƒç”»åƒ: {Path(result['comparison_image_path']).name}")
        
        logger.info(f"\nğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}")
        logger.info(f"  ğŸ–¼ï¸ ç”Ÿæˆç”»åƒæ•°: {total_images}æš")
        logger.info(f"  ğŸ“‹ æˆ¦ç•¥è©³ç´°: {len(test_parts)}ãƒ‘ãƒ¼ãƒ„åˆ†ã®JSON")
        
        # å¯è¦–åŒ–èª¬æ˜
        logger.info(f"\nğŸ¨ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯è¦–åŒ–ã®è¦‹æ–¹:")
        logger.info(f"  ğŸŸ¢ ç·‘ã®å††: æ­£ä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã“ã®é ˜åŸŸã‚’å«ã‚ã‚‹ï¼‰")
        logger.info(f"  ğŸ”´ èµ¤ã®å††: è² ä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã“ã®é ˜åŸŸã‚’é™¤å¤–ã™ã‚‹ï¼‰")
        logger.info(f"  âšª ç™½ã„ç¸: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¢ƒç•Œ")
        logger.info(f"  ğŸ”¢ æ•°å­—: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç•ªå·ï¼ˆé…ç½®é †åºï¼‰")
        
        logger.info("ğŸ‰ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯è¦–åŒ–ãƒ†ã‚¹ãƒˆå®Œäº†!")
        
    except Exception as e:
        logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()