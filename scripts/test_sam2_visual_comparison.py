#!/usr/bin/env python3
"""
SAM2ãƒ¢ãƒ‡ãƒ«è¦–è¦šçš„æ¯”è¼ƒãƒ†ã‚¹ãƒˆ

å„ãƒ¢ãƒ‡ãƒ«ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’è¦–è¦šçš„ã«æ¯”è¼ƒè¡¨ç¤º
"""

import sys
import cv2
import numpy as np
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.utils import setup_logging, get_logger
from src.core.sam2.sam2_model import SAM2ModelManager
from src.core.sam2.prompt_handler import SAM2PromptHandler, Live2DPromptPresets

def add_text_with_background(
    image: np.ndarray, 
    text: str, 
    position: Tuple[int, int], 
    font_scale: float = 0.7,
    thickness: int = 2,
    text_color: Tuple[int, int, int] = (255, 255, 255),
    bg_color: Tuple[int, int, int] = (0, 0, 0),
    padding: int = 5
) -> None:
    """èƒŒæ™¯ä»˜ããƒ†ã‚­ã‚¹ãƒˆã‚’ç”»åƒã«è¿½åŠ """
    
    font = cv2.FONT_HERSHEY_SIMPLEX
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’è¨ˆç®—
    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)
    
    x, y = position
    
    # èƒŒæ™¯çŸ©å½¢ã‚’æç”»
    bg_x1 = x - padding
    bg_y1 = y - text_height - padding
    bg_x2 = x + text_width + padding
    bg_y2 = y + baseline + padding
    
    cv2.rectangle(image, (bg_x1, bg_y1), (bg_x2, bg_y2), bg_color, -1)
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”»
    cv2.putText(image, text, (x, y), font, font_scale, text_color, thickness)

def create_segmentation_overlay(
    image_rgb: np.ndarray,
    masks: np.ndarray,
    scores: np.ndarray,
    part_name: str
) -> np.ndarray:
    """ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ç”»åƒã‚’ä½œæˆ"""
    
    # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒã‚¹ã‚¯ã‚’ä½¿ç”¨
    best_mask_idx = np.argmax(scores)
    best_mask = masks[best_mask_idx].astype(bool)
    best_score = scores[best_mask_idx]
    
    # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ç”»åƒä½œæˆ
    overlay = image_rgb.copy().astype(np.float32)
    
    # éƒ¨ä½åˆ¥ã®è‰²è¨­å®š
    colors = {
        "hair": [255, 100, 100],  # èµ¤ç³»
        "face": [100, 255, 100],  # ç·‘ç³»
        "body": [100, 100, 255],  # é’ç³»
    }
    
    color = np.array(colors.get(part_name, [255, 255, 100]))
    
    # ãƒã‚¹ã‚¯éƒ¨åˆ†ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    overlay[best_mask] = overlay[best_mask] * 0.6 + color * 0.4
    
    return overlay.astype(np.uint8)

def test_single_model_with_visualization(
    model_name: str,
    image_path: Path,
    image_rgb: np.ndarray,
    test_prompts: Dict[str, List],
    output_dir: Path
) -> Dict[str, Any]:
    """å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆ + å¯è¦–åŒ–çµæœä½œæˆ"""
    logger = get_logger(__name__)
    
    try:
        # SAM2ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        start_time = time.time()
        sam2_manager = SAM2ModelManager(model_name=model_name)
        if not sam2_manager.load_model():
            return {"error": f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {model_name}"}
        
        model_load_time = time.time() - start_time
        
        results = {
            "model_name": model_name,
            "image_name": image_path.stem,
            "model_load_time": model_load_time,
            "part_results": {},
            "visualizations": {}
        }
        
        height, width = image_rgb.shape[:2]
        
        # å„éƒ¨ä½ã§ãƒ†ã‚¹ãƒˆ
        for part_name, prompts in test_prompts.items():
            logger.info(f"  {part_name} ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ - {model_name}")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
            handler = SAM2PromptHandler()
            handler.start_new_session()
            
            for prompt in prompts:
                handler.add_point_prompt(
                    prompt.x, prompt.y, prompt.label,
                    description=prompt.description
                )
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            start_time = time.time()
            sam2_prompts = handler.get_sam2_prompts()
            masks, scores, logits = sam2_manager.predict(
                image=image_rgb,
                **sam2_prompts,
                multimask_output=True
            )
            inference_time = time.time() - start_time
            
            # çµæœè¨˜éŒ²
            best_score = float(np.max(scores))
            best_mask = masks[np.argmax(scores)].astype(bool)
            mask_coverage = float(np.sum(best_mask) / best_mask.size)
            
            results["part_results"][part_name] = {
                "score": best_score,
                "inference_time": inference_time,
                "mask_coverage": mask_coverage,
                "num_masks": len(masks)
            }
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®å¯è¦–åŒ–
            overlay_image = create_segmentation_overlay(image_rgb, masks, scores, part_name)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‚¹ã‚’æç”»
            prompt_coords = sam2_prompts["point_coords"]
            if prompt_coords is not None:
                for i, (x, y) in enumerate(prompt_coords):
                    cv2.circle(overlay_image, (int(x), int(y)), 8, (255, 255, 0), -1)
                    cv2.circle(overlay_image, (int(x), int(y)), 10, (0, 0, 0), 2)
                    
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç•ªå·
                    add_text_with_background(
                        overlay_image, str(i+1), 
                        (int(x)+15, int(y)), 
                        font_scale=0.6, 
                        text_color=(255, 255, 0),
                        bg_color=(0, 0, 0)
                    )
            
            # ã‚¹ã‚³ã‚¢è¡¨ç¤º
            score_text = f"{part_name}: {best_score:.3f}"
            add_text_with_background(
                overlay_image, score_text,
                (10, 30 + list(test_prompts.keys()).index(part_name) * 35),
                font_scale=0.8,
                text_color=(255, 255, 255),
                bg_color=(0, 0, 0)
            )
            
            results["visualizations"][part_name] = overlay_image
            
            logger.info(f"    ã‚¹ã‚³ã‚¢: {best_score:.3f}, æ™‚é–“: {inference_time:.2f}s")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        sam2_manager.unload_model()
        
        return results
        
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆå¤±æ•— {model_name}: {e}")
        return {"error": str(e)}

def create_model_comparison_grid(
    image_rgb: np.ndarray,
    all_results: List[Dict[str, Any]],
    part_name: str,
    output_path: Path
) -> None:
    """ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚°ãƒªãƒƒãƒ‰ç”»åƒã‚’ä½œæˆ"""
    logger = get_logger(__name__)
    
    try:
        models = ["sam2_hiera_tiny.pt", "sam2_hiera_small.pt", "sam2_hiera_base_plus.pt", "sam2_hiera_large.pt"]
        model_short_names = ["Tiny", "Small", "Base+", "Large"]
        
        # 2x2ã‚°ãƒªãƒƒãƒ‰ã§çµæœè¡¨ç¤º
        height, width = image_rgb.shape[:2]
        grid_image = np.zeros((height * 2, width * 2, 3), dtype=np.uint8)
        
        positions = [(0, 0), (0, 1), (1, 0), (1, 1)]
        
        for idx, (model_name, short_name, pos) in enumerate(zip(models, model_short_names, positions)):
            row, col = pos
            y_start, y_end = row * height, (row + 1) * height
            x_start, x_end = col * width, (col + 1) * width
            
            # å¯¾å¿œã™ã‚‹çµæœã‚’æ¤œç´¢
            model_result = None
            for result in all_results:
                if result.get("model_name") == model_name and "error" not in result:
                    model_result = result
                    break
            
            if model_result and part_name in model_result.get("visualizations", {}):
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœç”»åƒã‚’ä½¿ç”¨
                model_image = model_result["visualizations"][part_name].copy()
                
                # ãƒ¢ãƒ‡ãƒ«åã¨ã‚¹ã‚³ã‚¢ã‚’ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒ¼ã«è¡¨ç¤º
                score = model_result["part_results"][part_name]["score"]
                inference_time = model_result["part_results"][part_name]["inference_time"]
                
                title_text = f"{short_name} - Score: {score:.3f}"
                time_text = f"Time: {inference_time:.2f}s"
                
                # ã‚¿ã‚¤ãƒˆãƒ«èƒŒæ™¯
                title_bg_height = 80
                cv2.rectangle(model_image, (0, 0), (width, title_bg_height), (0, 0, 0), -1)
                
                # ã‚¿ã‚¤ãƒˆãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
                add_text_with_background(
                    model_image, title_text,
                    (10, 25),
                    font_scale=0.8,
                    text_color=(255, 255, 255),
                    bg_color=(0, 0, 0),
                    padding=0
                )
                
                add_text_with_background(
                    model_image, time_text,
                    (10, 55),
                    font_scale=0.6,
                    text_color=(200, 200, 200),
                    bg_color=(0, 0, 0),
                    padding=0
                )
                
            else:
                # ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯çµæœãªã—ã®å ´åˆ
                model_image = image_rgb.copy()
                
                # ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
                error_text = f"{short_name} - ERROR"
                add_text_with_background(
                    model_image, error_text,
                    (width//2 - 100, height//2),
                    font_scale=1.0,
                    text_color=(255, 255, 255),
                    bg_color=(0, 0, 255)
                )
            
            # ã‚°ãƒªãƒƒãƒ‰ã«é…ç½®
            grid_image[y_start:y_end, x_start:x_end] = model_image
        
        # ä¿å­˜
        grid_bgr = cv2.cvtColor(grid_image, cv2.COLOR_RGB2BGR)
        cv2.imwrite(str(output_path), grid_bgr)
        logger.info(f"æ¯”è¼ƒã‚°ãƒªãƒƒãƒ‰ä¿å­˜: {output_path}")
        
    except Exception as e:
        logger.error(f"ã‚°ãƒªãƒƒãƒ‰ä½œæˆå¤±æ•—: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # ãƒ­ã‚°è¨­å®š
    setup_logging(level="INFO", console_output=True, structured=False)
    logger = get_logger(__name__)
    
    logger.info("=== SAM2ãƒ¢ãƒ‡ãƒ«è¦–è¦šçš„æ¯”è¼ƒãƒ†ã‚¹ãƒˆ ===")
    
    try:
        # ãƒ†ã‚¹ãƒˆç”»åƒå–å¾—
        sample_images_path = project_root / "data" / "samples" / "demo_images"
        image_files = list(sample_images_path.glob("*.png"))
        
        if not image_files:
            logger.error("âŒ ãƒ†ã‚¹ãƒˆç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
        output_dir = project_root / "data" / "output" / "sam2_visual_comparison"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ†ã‚¹ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚2ã¤ã«é™å®šï¼‰
        models_to_test = [
            "sam2_hiera_tiny.pt",
            "sam2_hiera_small.pt", 
            "sam2_hiera_base_plus.pt",
            "sam2_hiera_large.pt"
        ]
        
        # æœ€åˆã®2æšã®ç”»åƒã§ãƒ†ã‚¹ãƒˆ
        for img_idx, image_path in enumerate(image_files[:2]):
            logger.info(f"\\nğŸ“¸ ç”»åƒ {img_idx + 1}/2: {image_path.name}")
            
            # ç”»åƒèª­ã¿è¾¼ã¿
            image = cv2.imread(str(image_path))
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            height, width = image_rgb.shape[:2]
            
            logger.info(f"   ç”»åƒã‚µã‚¤ã‚º: {width}x{height}")
            
            # Live2Dãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™
            test_prompts = {
                "hair": Live2DPromptPresets.hair_segmentation((height, width)),
                "face": Live2DPromptPresets.face_segmentation((height, width)),
                "body": Live2DPromptPresets.body_segmentation((height, width))
            }
            
            image_results = []
            
            # å„ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
            for model_name in models_to_test:
                logger.info(f"  ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model_name}")
                
                result = test_single_model_with_visualization(
                    model_name, image_path, image_rgb, test_prompts, output_dir
                )
                
                if "error" not in result:
                    image_results.append(result)
                    
                    # å„éƒ¨ä½ã®çµæœè¡¨ç¤º
                    for part_name, part_result in result["part_results"].items():
                        logger.info(f"    {part_name}: ã‚¹ã‚³ã‚¢ {part_result['score']:.3f}, "
                                  f"æ™‚é–“ {part_result['inference_time']:.2f}s")
                else:
                    logger.error(f"    âŒ {result['error']}")
            
            # å„éƒ¨ä½åˆ¥ã®æ¯”è¼ƒã‚°ãƒªãƒƒãƒ‰ä½œæˆ
            if image_results:
                for part_name in ["hair", "face", "body"]:
                    grid_path = output_dir / f"{part_name}_comparison_{image_path.stem}.png"
                    create_model_comparison_grid(image_rgb, image_results, part_name, grid_path)
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        logger.info(f"\\nğŸ‰ è¦–è¦šçš„æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Œäº†!")
        logger.info(f"çµæœ: {output_dir}")
        logger.info(f"  - éƒ¨ä½åˆ¥æ¯”è¼ƒ: [part]_comparison_[image].png")
        logger.info(f"  - hair: é«ªã®æ¯›ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¯”è¼ƒ")
        logger.info(f"  - face: é¡”ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¯”è¼ƒ") 
        logger.info(f"  - body: ä½“ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¯”è¼ƒ")
        
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()